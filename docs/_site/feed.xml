<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2021-11-24T17:30:22+08:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Data Privacy and Trustworthy Machine Learning Research Lab</title><subtitle>Website of the NUS Data Privacy and Trustworthy Machine Learning Research Lab.</subtitle><entry><title type="html">DP Dynamics of Noisy Gradient Descent</title><link href="http://0.0.0.0:4000/jekyll/update/2021/11/22/DPDynamics.html" rel="alternate" type="text/html" title="DP Dynamics of Noisy Gradient Descent" /><published>2021-11-22T11:00:00+08:00</published><updated>2021-11-22T11:00:00+08:00</updated><id>http://0.0.0.0:4000/jekyll/update/2021/11/22/DPDynamics</id><content type="html" xml:base="http://0.0.0.0:4000/jekyll/update/2021/11/22/DPDynamics.html">&lt;p style=&quot;display: none;&quot;&gt; 
$$
\newcommand{\D}{D} % dataset / database
\newcommand{\smh}{\beta}                                     % Smoothness parameter
\newcommand{\loss}[2]{\ell(#2;\mathbf{#1})}                  % Small loss \loss; inputs are fine
\newcommand{\ptheta}{\theta}                                 % Removed input and shortened
\newcommand{\thet}{\Theta}                                 % Removed input and shortened
\newcommand{\x}{\mathbf{x}}
\newcommand{\K}{K}					     % Number of steps
\newcommand{\T}{T}					     % Number of steps
\newcommand{\kk}{k}
\newcommand{\proj}[2]{\Pi_{#1}(#2)}
\newcommand{\C}{\mathcal{C}}
\newcommand{\sig}{\sigma}
\newcommand{\q}{\alpha}
\newcommand{\eps}{\varepsilon} % Define epsion
\newcommand{\size}{n}                                 	     % Database size
\newcommand{\step}{\eta}
\newcommand{\R}{\mathbb{R}} % Notation for Real numbers
\newcommand{\Univ}{\mathcal{X}} % Universe of all database records.
\newcommand{\thetaspace}{\mathbb{R}^d}
\newcommand{\ve}{\mathbf v}
\newcommand{\Loss}{\mathcal{L}}                              % Big loss \Loss; inputs not necessary
\newcommand{\Domain}{\Univ^\size} % Set of all possible databases
\newcommand{\Ren}[3]{R_{#1}\left(#2\middle\|#3\right)}       % Sortened the name
\newcommand{\deltW}[1]{\dif{\W{#1}}}
\newcommand{\graD}[1]{\nabla #1}
\newcommand{\hesS}[1]{\nabla^2 #1}
\newcommand{\tra}[1]{\textsf{Tr}\left(#1\right)}
\newcommand{\dif}[1]{d #1}
\newcommand{\der}[2]{\frac{\dif{#1}}{\dif{#2}}}
\newcommand{\doh}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\lapL}[1]{\Delta #1}
\newcommand{\divR}[1]{\nabla \cdot #1}
\newcommand{\W}[1]{\mathbf{W}_{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Expec}[2]{\underset{#1}{\E}\left[#2\right]}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\Fren}[3]{E_{#1}\left(#2\middle\|#3\right)}      % Changed name
\newcommand{\Gren}[3]{I_{#1}\left(#2\middle\|#3\right)}
\newcommand{\sen}[1]{S_{#1}}
$$
&lt;/p&gt; 
&lt;p&gt;What is the information leakage of an iterative randomized learning algorithm about its training data, when the internal state of the algorithm is private? How much is the contribution of each specific training epoch to the information leakage through the released model? We study this problem for noisy gradient descent algorithms, and model the dynamics of Rényi differential privacy loss throughout the training process. Our analysis traces a provably tight bound on the Rényi divergence between the pair of probability distributions over parameters of models trained on neighboring datasets. We prove that the privacy loss converges exponentially fast, for smooth and strongly convex loss functions, which is a significant improvement over composition theorems (which over-estimate the privacy loss by upper-bounding its total value over all intermediate gradient computations). For Lipschitz, smooth, and strongly convex loss functions, we prove optimal utility with a small gradient complexity for noisy gradient descent algorithms. We provide additional details about our analysis in the &lt;a href=&quot;https://arxiv.org/pdf/2102.05855.pdf&quot;&gt;full version &amp;gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;!-- &lt;d-cite key=&quot;&quot;&gt;&lt;/d-cite&gt; --&gt;
&lt;p&gt;Machine learning models leak a significant amount of information about their training data, through their parameters and predictions&lt;d-cite key=&quot;shokri2017membership, nasr2019comprehensive, carlini2020extracting&quot;&gt;&lt;/d-cite&gt;.  Iterative randomized training algorithms can limit this information leakage and bound the differential privacy loss of the learning process &lt;d-cite key=&quot;bassily2014private, abadi2016deep, feldman2018privacy, feldman2020private&quot;&gt;&lt;/d-cite&gt;.  The strength of this certified defense is determined by an &lt;em&gt;upper bound&lt;/em&gt; on the (Rényi) divergence between the probability distributions of model parameters learned on any pair of neighboring datasets.&lt;/p&gt;
&lt;p&gt;The general method to compute the differential privacy bound for gradient perturbation-based learning algorithms is to view the process as a number of (identical) differential privacy mechanisms, and to compute the &lt;em&gt;composition&lt;/em&gt; of their bounds.  However, this over-estimates the privacy loss of the released model&lt;d-cite key=&quot;jagielski2020auditing, nasr2021adversary&quot;&gt;&lt;/d-cite&gt;, and results in a loose differential privacy bound. This is because composition bounds also accounts for the leakage of all intermediate gradient updates, even though only the final model parameters are observable to adversary. Feldman et al.&lt;d-cite key=&quot;feldman2018privacy, feldman2020private&quot;&gt;&lt;/d-cite&gt; address this issue for the privacy analysis of gradient computations over &lt;em&gt;one single&lt;/em&gt; training epoch, for smooth and convex loss functions.  However, in learning a model over multiple training epochs, such a guarantee is quantitatively similar to composition bounds of privacy amplification by sub-sampling&lt;d-cite key=&quot;feldman2018privacy&quot;&gt;&lt;/d-cite&gt;. The open challenge, that we tackle in this paper, is to provide an analysis that can tightly bound the privacy loss of the &lt;em&gt;released model&lt;/em&gt; after $K$ training epochs, for any $K$.&lt;/p&gt;
&lt;p&gt;We present a novel analysis for privacy dynamics of noisy gradient descent with smooth and strongly convex loss functions. We construct a pair of continuous-time Langevin diffusion&lt;d-cite key=&quot;sato2014approximation&quot;&gt;&lt;/d-cite&gt; processes that trace the probability distributions over the model parameters of noisy GD.  Subsequently, we derive differential inequalities bounding the &lt;em&gt;rate of privacy loss&lt;/em&gt; (worst case Rényi divergence between the &lt;strong&gt;coupled stochastic processes&lt;/strong&gt; associated with neighboring datasets) throughout the training process. Using it, we prove an exponentially-fast converging privacy bound for noisy GD:&lt;/p&gt;
&lt;div id=&quot;main_theorem&quot; class=&quot;theorem&quot;&gt;&lt;b&gt;1.&lt;/b&gt;
Under $\lambda$-strongly convex and $\smh$-smooth loss function $\loss{\x}{\ptheta}$ with gradient sensitivity $\sen{g}$, the &lt;a href=&quot;#goal-noisygd&quot;&gt;noisy GD Algorithm 1&lt;/a&gt; for $\K$ steps, with initial parameter vector $\ptheta_0\sim\proj{\C}{\mathcal{N}(0,2\sig^2\mathbb{I}_d)}$, step size $\step &lt; \frac{1}{\smh}$, and noise variance $\sig^2$, satisfies $(\q, \eps)$-Rényi DP with $\eps = \frac{\q\sen{g}^2}{\lambda\sigma^2\size^2}(1-e^{-\step\frac{K}{2}})$, where $n$ is the size of the training set.
&lt;/div&gt;
&lt;aside markdown=&quot;1&quot;&gt;
Here $\C$ is any convex set in $\R^d$ and the projection operation is defined as $\proj{\C}{\ptheta} \triangleq \underset{\ptheta' \in \C}{\arg \min} \lVert \ptheta - \ptheta' \rVert$.
&lt;/aside&gt;
&lt;p&gt;This guarantee shows that the privacy loss &lt;strong&gt;converges&lt;/strong&gt; exponentially in the number of iterations $K$, instead of growing proportionally with $K$ as in the composition-based analysis of the same algorithms.  Our bound captures a strong privacy amplification due to the dynamics (and convergence) of differential privacy over the noisy gradient descent algorithm with private internal state.&lt;/p&gt;
&lt;!-- #####  --&gt;
&lt;div class=&quot;remark&quot;&gt;
We also prove that this &lt;b&gt;bound is tight&lt;/b&gt; by showing that there exist a loss function and neighboring datasets such that the divergence between corresponding model parameter distributions grows at a matching order. As a consequence of this improved privacy bound, we prove that noisy GD achieves &lt;b&gt;optimal utility&lt;/b&gt; under differential privacy with an error of order $O(\frac{d}{n^2\eps^2})$, with a &lt;i&gt;small gradient complexity&lt;/i&gt; of order $O(n\log(n))$. This is a significant improvement over the prior utility analysis for noisy SGD algorithms&lt;d-cite key=&quot;bassily2014private&quot;&gt;&lt;/d-cite&gt;, wherein the gradient complexity reduces by a factor of ${\size/\log(\size)}$, and attainable utility improves by a factor of $\text{poly}\log(\size)$. 
&lt;/div&gt;
&lt;h2 id=&quot;problem-formulation&quot;&gt;Problem Formulation&lt;/h2&gt;
&lt;p&gt;Let $\D = (\x_1, \x_2, \cdots, \x_\size)$ be a dataset of size $\size$ with records taken from a universe $\Univ$. For a given machine learning algorithm, let ${\loss{\x}{\ptheta} : \Univ \times \thetaspace \rightarrow \R}$ be a loss function of a parameter vector $\ptheta \in \C$ on the data point $\x$, where $\C$ is a closed convex set (can be $\mathbb{R}^d$).&lt;/p&gt;
&lt;p&gt;A generic formulation of the optimization problem to learn the model parameters, is in the form of empirical risk minimization (ERM) with the following objective, where $\Loss_\D(\ptheta)$ is the empirical loss of the model, with parameter vector $\ptheta$, on a dataset $\D$.&lt;/p&gt;
&lt;p&gt;$$
\ptheta^* = \underset{\ptheta \in \C}{\arg \min}~\Loss_\D(\ptheta), \quad \text{where} \quad \Loss_\D(\ptheta)=\frac{1}{\size} \sum_{\x \in \D}\loss{\x}{\ptheta}.
$$&lt;/p&gt;
&lt;p&gt;Releasing this optimization output (i.e., $\ptheta^*$) can leak information about the dataset $\D$, hence violating data privacy. To mitigate this risk, there exist randomized algorithms to ensure that the ($\q$-Rényi) privacy loss of the ERM algorithm is upper-bounded by $\eps$, i.e., the algorithm satisfies ${(\q, \eps)\text{-RDP}}$.&lt;/p&gt;
&lt;aside markdown=&quot;1&quot;&gt;
A randomized algorithm $\mathcal A: \Domain \rightarrow \thetaspace$ satisfies &lt;b&gt;$(\q, \eps)$-Rényi Differential Privacy&lt;/b&gt; (RDP), if for any two neighboring datasets $\D, \D' \in \Domain$, the $\alpha$-Rényi divergence is bounded: 
$\Ren{\q}{\mathcal A(\D)}{\mathcal A(\D')} \leq \eps$.
&lt;/aside&gt;
&lt;div id=&quot;renyi_differential_privacy&quot;&gt;
&lt;img class=&quot;img-fluid rounded z-depth-1&quot;  src=&quot;/assets/resized/renyi_differential_privacy-800x284.png&quot; srcset=&quot;    /assets/resized/renyi_differential_privacy-480x171.png 480w,    /assets/resized/renyi_differential_privacy-800x284.png 800w,/ assets/2021-11-22-DPDynamics/renyi_differential_privacy.png 847w&quot;   data-zoomable/&gt;
&lt;/div&gt;
&lt;p&gt;In this paper, our objective is to model and analyze the &lt;strong&gt;dynamics of differential privacy&lt;/strong&gt; of a popular randomized ERM algorithm called &lt;em&gt;Noisy Gradient Descent&lt;/em&gt;&lt;d-cite key=&quot;bassily2014private&quot;&gt;&lt;/d-cite&gt; (&lt;a href=&quot;#goal-noisygd&quot;&gt;Algorithm 1&lt;/a&gt;).&lt;/p&gt;
&lt;pre id=&quot;read-noisygd&quot; style=&quot;display:none;&quot;&gt;
\begin{algorithm}
\caption{$\mathcal A_{\text{Noisy-GD}}$: Noisy Gradient Descent}
\begin{algorithmic}
\PROCEDURE{Noisy-GD}{Dataset $D = (\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n)$, loss function $\ell(\theta;\mathbf{x})$, closed convex set $\mathcal{C}\subseteq\mathbb{R}^d$, learning rate $\eta$, noise variance $\sigma^2$, initial parameter vector $\theta$}
    \FOR{$k = 0, 1, \cdots, K - 1$} 
        \STATE {$g(\theta_k;D)= \sum_{i=1}^n \nabla\ell({\theta_k};{\mathbf{x}_i})$}
        \STATE {$\theta_{k+1} = \Pi_{\mathcal C} (\theta_{k} - \frac{\eta}{n} g(\theta_k;D) + \sqrt{2\eta} \mathcal{N}(0, \sigma^2 \mathbb{I}_d))$}
    \ENDFOR
    \STATE {Output $\theta_{K}$}
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
&lt;/pre&gt;
&lt;div id=&quot;goal-noisygd&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
    var code = document.getElementById(&quot;read-noisygd&quot;).textContent;
    var parentEl = document.getElementById(&quot;goal-noisygd&quot;);
    var options = {
        lineNumber: true
    };
    pseudocode.render(code, parentEl, options);
&lt;/script&gt;
&lt;p&gt;More precisely, we focus on the following.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute an RDP bound (i.e., the worst case Rényi divergence $\Ren{\q}{\Theta_K}{\Theta_K'}$ between the output distributions of two neighboring datasets) for &lt;a href=&quot;#goal-noisygd&quot;&gt;Algorithm 1&lt;/a&gt;, and analyze its tightness.&lt;/li&gt;
&lt;li&gt;Compute the contribution of each iteration to the privacy loss.  As we go from step $k = 1$ to $K$ in &lt;a href=&quot;#goal-noisygd&quot;&gt;Algorithm 1&lt;/a&gt;, we investigate how the algorithm's privacy loss changes as it runs the $k$'th iteration (computed as ${\Ren{\q}{\Theta_{k}}{\Theta_{k}'} - \Ren{\q}{\Theta_{(k-1)}}{\Theta_{(k-1)}'}}$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We emphasize that our goal is to construct a theoretical framework for analyzing privacy loss of releasing the output $\ptheta_K$ of the algorithm, assuming &lt;em&gt;private&lt;/em&gt; internal states (i.e., $\ptheta_1, \cdots, \ptheta_{K-1}$). In the end, we aim to provide a RDP bound that is tight, thus facilitating optimal empirical risk minimization with differential privacy&lt;d-cite key=&quot;bassily2014private&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt;
&lt;h2 id=&quot;privacy-analysis&quot;&gt;Privacy Analysis&lt;/h2&gt;
&lt;h3 id=&quot;tracing-diffusion-for-noisy-gd&quot;&gt;Tracing Diffusion for Noisy GD&lt;/h3&gt;
&lt;p&gt;To analyze the privacy loss of &lt;a href=&quot;#goal-noisygd&quot;&gt;noisy GD&lt;/a&gt;, which is a &lt;em&gt;discrete-time stochastic process&lt;/em&gt;, we first interpolate each discrete update from $\ptheta_k$ to $\ptheta_{k+1}$ with a piece-wise continuously differentiable diffusion process. Let $\D$ and $\D'$ be a pair of arbitrarily chosen neighboring datasets.
Given step-size $\step$ and initial parameter vector $\ptheta_0=\ptheta_0'$, the respective $k$'th discrete updates in &lt;a href=&quot;#goal-noisygd&quot;&gt;Algorithm 1&lt;/a&gt; on neighboring datasets $D$ and $D'$ are
$$
\begin{aligned}
\ptheta_{k+1} &amp;amp;= \proj{\C}{\ptheta_{k} - \step \nabla{\Loss_\D(\ptheta_k)} + \sqrt{2\step\sig^2} Z_k},\\
\ptheta_{k+1}' &amp;amp;= \proj{\C}{\ptheta_{k}'-\step\nabla{\Loss_\D(\ptheta_k')}+ \sqrt{2\step\sig^2} Z_k },
\end{aligned}
$$
with $Z_k \sim \mathcal{N}(0,\mathbb{I}_d)$.&lt;/p&gt;
&lt;p&gt;These two discrete jumps can be interpolated with two stochastic processes $\thet_t$ and $\thet_t'$ over time $\step k \leq t\leq\step (k+1)$ respectively (visualized below via dotted lines).&lt;/p&gt;
&lt;figure id=&quot;coupled_diffusion&quot;&gt;
&lt;img class=&quot;img-fluid rounded z-depth-1&quot;  src=&quot;/assets/resized/coupled_diffusion-800x417.png&quot; srcset=&quot;    /assets/resized/coupled_diffusion-480x250.png 480w,    /assets/resized/coupled_diffusion-800x417.png 800w,/ assets/2021-11-22-DPDynamics/coupled_diffusion.png 1298w&quot;   data-zoomable/&gt;
&lt;/figure&gt;
&lt;p&gt;At the start of each step, $t=\eta k$, the random variables $\thet_{\eta k}$ and $\thet_{\eta k}'$ model the distribution of the $\theta_k$ and $\theta_k'$ in the noisy GD processes respectively. Immediately after $t=\eta k$, the random variables undergo identical mapping $\phi(\cdot)$ defined as $\phi(\theta) = \theta - \eta U_+(\theta)$. During time $\eta k&amp;lt; t &amp;lt;\eta (k+1)$, the two process diffuse along opposing vector fields ($U_-(\cdot)$ and $-U_-(\cdot)$) with Brownian perturbation. At the end of step, i.e. at $t \rightarrow \eta (k+1)$, we project $\thet_t$ and $\thet_t'$ onto convex set $\C$, and obtain $\thet_{\eta (k + 1)}$ and $\thet_{\eta (k + 1)}'$.&lt;/p&gt;
&lt;p&gt;Repeating the construction for $k=0,\cdots,K-1$, we define two piece-wise continuous diffusion processes $\{ \thet_t \}_{t \geq 0}$ and $\{ \thet_t' \}_{t \geq 0}$ whose distributions at time $t=\step k$ are consistent with $\theta_k$ and $\theta_{k}'$ in the noisy GD processes for any $k \in \{0,\cdots, K-1\}$.&lt;/p&gt;
&lt;div class=&quot;definition&quot;&gt;&lt;b&gt;(Coupled tracing diffusions).&lt;/b&gt; 
Let $\thet_0=\thet_0'$ be two identically distributed random variables. We refer to the stochastic processes $\{ \thet_t \}_{t \geq 0}$ and $\{ \thet_t' \}_{t \geq 0}$ that evolve along &lt;a href=&quot;#coupled_diffusion&quot;&gt;diffusion processes&lt;/a&gt; in $\eta k&lt; t &lt; \eta (k+1)$ and undergo projection steps $\proj{\C}{\cdot}$ at the end of step $t = \step(k+1)$, as coupled tracing diffusions for noisy GD on neighboring datasets $\D,\D'$.    
&lt;/div&gt;
&lt;p&gt;The Rényi divergence $R_{\alpha}(\thet_{\eta \K}\lVert\thet_{\eta \K}')$ reflects the Rényi privacy loss of &lt;a href=&quot;#goal-noisygd&quot;&gt;noisy GD&lt;/a&gt; with $\K$ steps. Conditioned on observing $\ptheta_k$ and $\theta_\kk'$, the processes $\{\Theta_t\}_{\step\kk&amp;lt; t&amp;lt; \step(\kk+1)}$ and $\{\Theta_t'\}_{\step\kk&amp;lt; t&amp;lt;\step(\kk+1)}$ are &lt;em&gt;Langevin diffusions&lt;/em&gt; along vector fields $ - U_-(\ptheta_{\kk})$ and $U_-(\ptheta_{\kk}')$ respectively, for duration $\step$. That is, conditioned on observing $\theta_\kk$ and $\theta_\kk'$, the two diffusion processes have the following stochastic differential equations (SDEs) respectively.&lt;/p&gt;
&lt;p&gt;$$
\dif{\thet_t} = - U_-(\theta_k)\dif{t} + \sqrt{2\sig^2}\deltW{t}, \quad \dif{\thet_t}' =  U_-(\theta_k')\dif{t} + \sqrt{2\sig^2}\deltW{t},
$$&lt;/p&gt;
&lt;p&gt;where $\deltW{t}\sim\sqrt{dt}\mathcal N(0,I_d)$ describe the Wiener processes on $\thetaspace$.&lt;/p&gt;
&lt;figure id=&quot;renyi_differential_privacy&quot;&gt;
    &lt;img src=&quot;/assets/2021-11-22-DPDynamics/fokker_planck.gif&quot;&gt;
    &lt;figcaption&gt;Visualization of Fokker-Planck equation for Langevin diffusion by &lt;a href=&quot;https://twitter.com/gabrielpeyre/status/1433293610586234882&quot;&gt;Gabriel Pyeré&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The joint effect of the drag force (i.e $- U_-(\theta_k)$) and Brownian fluctuations on the (conditional) probability density $p(\thet_t = \ptheta | \thet_{\eta k} = \ptheta_k)$ of random variable $\Theta_t$ given $\Theta_{\eta k} = \ptheta_k$ is characterized through the Fokker-Planck equation&lt;d-cite key=&quot;fokker1914mittlere&quot;&gt;&lt;/d-cite&gt; below. For brevity, we use $p_{t|\eta \kk}(\ptheta|\ptheta_k)$ and $p_{t|\eta \kk}'(\ptheta|\ptheta_k')$ to represent the conditional probability density function $p(\thet_t = \ptheta | \thet_{\eta k} = \ptheta_k)$ and $p(\thet_t' = \ptheta | \thet_{\eta \kk}' = \ptheta_\kk')$ respectively.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\doh{p_{t|\step\kk}(\ptheta|\ptheta_\kk)}{t} &amp;amp;= \divR{\left(p_{t|\step\kk}(\ptheta|\ptheta_\kk) U_-(\theta_k)\right)} + \sig^2 \lapL{p_{t|\step\kk}(\ptheta|\ptheta_\kk)} \\
\doh{p_{t | \step \kk}'(\ptheta|\ptheta_\kk')}{t} &amp;amp;= - \divR{\left(p_{t|\step\kk}'(\ptheta|\ptheta_\kk')U_-(\theta_k')\right)} + \sig^2 \lapL{p_{t|\step\kk}'(\ptheta|\ptheta_\kk')}
\end{aligned}
$$&lt;/p&gt;
&lt;aside markdown=&quot;1&quot;&gt;
Given a smooth vector field $\ve :\C \rightarrow \R^d$, its 
&lt;b&gt;Divergence&lt;/b&gt; $\divR{\ve}: \C \rightarrow \R$ is $(\divR{\ve})(\ptheta) = \underset{i=1}{\overset{d}{\sum}} \doh{\ve_i(\ptheta)}{\ptheta_i}$.
&lt;/aside&gt;
&lt;p&gt;By taking expectations over probability density function $p_{\eta k}(\theta_\kk)$ or $p_{\eta k}'(\theta_\kk')$ on both sides of above equation, we obtain the partial differential equation that models the evolution of (unconditioned) probability density function $p_t(\ptheta)$ and $p_t'(\ptheta)$ in the coupled tracing diffusions.&lt;/p&gt;
&lt;aside markdown=&quot;1&quot;&gt;
Given a smooth function $\Loss:\C \rightarrow \R$, its &lt;b&gt;Laplacian&lt;/b&gt; $\lapL{\Loss}:\C \rightarrow \R$ is the trace of its Hessian $\hesS{\Loss}$, (i.e., $\lapL{\Loss(\ptheta)} = \tra{\hesS{\Loss(\ptheta)}}$.
&lt;/aside&gt;
&lt;div class=&quot;lemma&quot; id=&quot;GDupdate_fokker&quot;&gt;&lt;b&gt;1.&lt;/b&gt; 
For &lt;a href=&quot;#coupled_diffusion&quot;&gt;coupled tracing diffusion processes&lt;/a&gt; in time $\step\kk &lt; t &lt; \step(\kk+1)$, the equivalent Fokker-Planck equations are
$$
\begin{aligned}
\doh{p_t(\ptheta)}{t} =  \divR{(p_t(\ptheta)V_t(\theta))} + \sig^2\lapL{p_t(\ptheta)}, \\
\doh{p_t'(\ptheta)}{t} =  \divR{(p_t'(\ptheta)V_t'(\theta))} + \sig^2\lapL{p_t'(\ptheta)}, 
\end{aligned}
$$
where $V_t(\theta)= \Expec{\ptheta_\kk \sim p_{\step\kk|t}}{U_-(\ptheta_\kk)|\ptheta}$ and $V_t'(\theta) = - \Expec{\ptheta_\kk \sim p_{\step\kk|t}'}{U_-(\ptheta_\kk)|\ptheta}$ are time-dependent vector fields on $\mathbb{R}^d$.
&lt;/div&gt;
&lt;p&gt;Since both these vector fields $V_t$ and $V_t'$ are expectations over $U_-(\cdot)$, the loss gradient difference vector between the two databases, it is easy to see that their magnitude is bounded by the sensitivity of total gradient.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\norm{V_t(\ptheta)} &amp;amp;\leq \frac{1}{2} \Expec{\ptheta_k \sim p_{\step k | t}}{\norm{\graD{\Loss_\D(\ptheta_k)} - \graD{\Loss_{\D'}(\ptheta_k)} } | \ptheta} \\
&amp;amp;\leq \frac{1}{2\size} \underbrace{\underset{\x, \x' \in \Univ}{\max} \underset{\ptheta \in \thetaspace}{\max}  \norm{\graD{\loss{\x}{\ptheta}} - \graD{\loss{\x'}{\ptheta}}}}_{\text{Sensitivity $\sen{g}$ of loss gradient}}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;As a result, the magnitude of the vector fields is also bounded, i.e. $\norm{V_t(\ptheta) - V_t'(\ptheta)} \leq \frac{\sen{g}}{\size}$ for all $t \geq 0$ and $\ptheta \in \thetaspace$.&lt;/p&gt;
&lt;p&gt;Via these two density evolution equations, we analyze differential privacy dynamics of the noisy gradient descent updates along coupled tracing diffusions.&lt;/p&gt;
&lt;h3 id=&quot;privacy-erosion-along-tracing-diffusion&quot;&gt;Privacy erosion along tracing diffusion&lt;/h3&gt;
&lt;p&gt;The Rényi divergence (privacy loss) $\Ren{\q}{\thet_t}{\thet_t'}$ between coupled tracing diffusion processes increases over time, as the vector fields $V_t, V_t'$ underlying two processes are different.  We refer to this phenomenon as &lt;strong&gt;privacy erosion&lt;/strong&gt;. This increase is determined by the amount of change in the probability density functions for coupled tracing diffusions, characterized by the &lt;a href=&quot;#GDupdate_fokker&quot;&gt;Fokker-Planck equations&lt;/a&gt; for diffusions under different vector fields. Using it, we compute a bound on the rate (partial derivative) of $\Ren{\q}{\thet_t}{\thet_t'}$ over time in the following lemma, to model privacy erosion between two different diffusion processes.&lt;/p&gt;
&lt;div class=&quot;lemma&quot; id=&quot;renyi_loss_rate&quot;&gt;&lt;b&gt;2 (Rate of Rényi privacy loss).&lt;/b&gt; 
Let $V_t$ and $V_t'$ be two vector fields on $\thetaspace$ corresponding to a pair of arbitrarily chosen neighboring datasets $\D$ and $\D'$ of size $\size$, and let $\ell()$ be a loss function with gradient sensitivity $\sen{g}$. Then, for corresponding coupled diffusions $\{\thet_t\}_{t\geq0}$ and $\{\thet_t'\}_{t\geq0}$ under vector fields $V_t$ and $V_t'$ and noise variance $\sig^2$, the Rényi privacy loss rate at any $t\geq0$ is upper bounded by
$$
\doh{\Ren{\q}{\thet_{t}}{\thet_{t}'}}{t} \leq \frac{1}{\gamma}\frac{\q \sen{g}^2}{4\sig^2\size^2} - (1-\gamma)\sig^2\q \frac{\Gren{\q}{\thet_{t}}{\thet_{t}'}}{\Fren{\q}{\thet_{t}}{\thet_{t}'}},
$$
where $\gamma&gt;0$ is a tuning parameter that we can fix arbitrarily according to our need, $\Gren{\q}{\thet_{t}}{\thet_{t}'}$ is the &lt;b&gt;Rényi information&lt;/b&gt;&lt;d-cite key=&quot;vempala2019rapid&quot;&gt;&lt;/d-cite&gt;, and $\Fren{\q}{\thet_{t}}{\thet_{t}'}$ is the &lt;b&gt;$\alpha$-moment of the likelihood ratio&lt;/b&gt;&lt;d-cite key=&quot;abadi2016deep&quot;&gt;&lt;/d-cite&gt; between $\thet_{t}$ and $\thet_{t}'$.
&lt;/div&gt;
&lt;p&gt;Lemma 2 bounds the rate of privacy loss with various terms. Generally speaking, the term $\frac{\sen{g}^2}{4\sigma^2\size^2}$ bounds the worst-case privacy loss growth due to noisy gradient update, while the term $\frac{I_{\alpha}(\Theta_t\lVert\Theta_t')}{E_{\alpha}(\Theta_t\lVert\Theta_t')}$ amplifies our bound for the rate of privacy loss, as both $\Gren{\q}{\thet_{t}}{\thet_{t}'}$ and $\Fren{\q}{\thet_{t}}{\thet_{t}'}$ are always non-negative. Following is the breakdown of the terms in Lemma 2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\frac{S_g^2}{4\sigma^2n^2}$: This is the first term in the right-hand side of &lt;a href=&quot;#renyi_loss_rate&quot;&gt;our privacy loss rate bound&lt;/a&gt;. It quantifies the worst-case privacy loss due of one noisy gradient update in &lt;a href=&quot;#goal-noisygd&quot;&gt;noisy GD Algorithm&lt;/a&gt;. The term $\frac{S_g}{\size}$ is the sensitivity of average loss gradient $\Loss_D(\theta)$ over two neighboring datasets $D,D'$. The larger $S_g$ is, the further apart the parameters after the gradient descent updates on $D,D'$ could be. The term $\sigma^2$ is the variance of Gaussian noise. Because additive noise shrinks the expected trajectory difference in noisy GD updates, the larger $\sigma^2$ is, the more indistinguishable the distributions are, and therefore, the smaller the privacy loss (Rényi divergence) is.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\frac{I_{\alpha}(\Theta_t\lVert\Theta_t')}{E_{\alpha}(\Theta_t\lVert\Theta_t')}$: This term is the second term on the right-hand side of &lt;a href=&quot;#renyi_loss_rate&quot;&gt;our privacy loss rate bound&lt;/a&gt;, which originates from the derivative of $p_t,p_t'$ with regard to time $t$. The expression $I_{\alpha}/E_{\alpha}$ originates when we use the Fokker Planck equation to substitute the terms related to $\frac{\partial p_t}{\partial t}, \frac{\partial p_t'}{\partial t}$ in the partial derivative of $R_{\alpha}(\Theta_t\lVert\Theta_t')$ with respect to $t$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\gamma$ is a tuning constant to balance the privacy growth rate estimated using the above two terms, thus helping us tune the privacy loss accumulation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since the ratio $I_\q/E_\q$ is always positive by definition, the &lt;a href=&quot;#renyi_loss_rate&quot;&gt;Rényi divergence (privacy loss) rate&lt;/a&gt; is bounded by its first component (a constant) given any fixed $\q$. Therefore, this naïve privacy analysis gives linear RDP guarantee for Langevin diffusion, which resembles the moment accountant analysis&lt;d-cite key=&quot;abadi2016deep&quot;&gt;&lt;/d-cite&gt;. However, a tighter bound of Rényi privacy loss is possible with finer control of the ratio $I_\q/E_\q$.&lt;/p&gt;
&lt;h3 id=&quot;controlling-rnyi-privacy-loss-rate&quot;&gt;Controlling Rényi privacy loss rate&lt;/h3&gt;
&lt;p&gt;Although &lt;a href=&quot;#renyi_loss_rate&quot;&gt;Lemma 2&lt;/a&gt; bounds the Rényi privacy loss rate, the Rényi Information term $\Gren{\q}{\thet_{t}}{\thet_{t}'}$ depends on unknown distributions $\thet_{t}, \thet_{t}'$, and is intractable to control in general. Even with explicit expressions for distributions $\thet_{t}, \thet_{t}'$, the calculation would involve integration in $\thetaspace$ which is computationally prohibitive for large $d$. We show that when the loss function $\ell(\cdot)$ is $\lambda$-strongly convex and $\beta$-smooth, the $I_\q/E_\q$ term in &lt;a href=&quot;#renyi_loss_rate&quot;&gt;Lemma 2&lt;/a&gt; can be controlled along the coupled tracing diffusions. More specifically, by choosing a step size $\step \leq \frac{1}{\beta}$ and start parameter distribution $\thet_0, \thet_0' \sim \proj{\C}{\mathcal{N} (0, \sig^2 I_d)}$, we show that for any $t \geq 0$,&lt;/p&gt;
&lt;p&gt;$$
\frac{\Gren{\q}{\thet_t}{\thet_t'}}{\Fren{\q}{\thet_t}{\thet_t'}} \geq \frac{\lambda} {\q^2\sigma^2}\times \left[\Ren{\q}{\thet_t}{\thet_t'} + \q(\q - 1) \doh{\Ren{\q}{\thet_t}{\thet_t'}}{\q}\right].
$$&lt;/p&gt;
&lt;p&gt;We prove this inequality by showing that under the stated conditions, the coupled tracing diffusions satisfies the Gaussian isoperimetric inequality, also known as &lt;em&gt;log-Sobolev inequality&lt;/em&gt;&lt;d-cite key=&quot;bakry2013analysis&quot;&gt;&lt;/d-cite&gt; (LSI), with constant $\frac{\lambda}{2\sigma^2}$ for all $t\geq 0$. Vempala et al.&lt;d-cite key=&quot;vempala2019rapid&quot;&gt;&lt;/d-cite&gt; proved that the log-Sobolev inequality can be equivalently stated as the bound above on $I_\q/E_\q$.&lt;/p&gt;
&lt;p&gt;On plugging the above inequality in &lt;a href=&quot;#renyi_loss_rate&quot;&gt;Lemma 2&lt;/a&gt;, we get the following partial differential inequation (PDI) that models the &lt;strong&gt;dynamics of Rényi privacy loss&lt;/strong&gt;, i.e. it describes how the Rényi privacy loss changes as a function of time $t$ and $\alpha$ during the interval $\eta k &amp;lt; t &amp;lt; \eta (k + 1)$. For brevity, let $R(\q,t)$ represent $\Ren{\q}{\thet_t}{\thet_t'}$. We have
$$
\doh{R(\q,t)}{t} \leq \frac{1}{\gamma}\frac{\q S_g^2}{4\sig^2\size^2} - \lambda(1-\gamma) \left[ \frac{R(\q,t)}{\q} + (\q-1)\doh{R(\q,t)}{\q} \right].
$$&lt;/p&gt;
&lt;p&gt;As per our &lt;a href=&quot;#coupled_diffusion&quot;&gt;coupled tracing diffusion&lt;/a&gt; process, no privacy loss is incurred at integral multiples of $\step$ as both the mapping $\phi(\cdot)$ and $\proj{\C}{\cdot}$ are identical for the coupled processes, and therefore can be seen as post-processing step (as shown below).&lt;/p&gt;
&lt;figure id=&quot;privacy_loss_one_step&quot;&gt;
&lt;img class=&quot;img-fluid rounded z-depth-1&quot;  src=&quot;/assets/resized/privacy_loss_one_step-480x171.png&quot; srcset=&quot;    /assets/resized/privacy_loss_one_step-480x171.png 480w,/ assets/2021-11-22-DPDynamics/privacy_loss_one_step.png 731w&quot;   data-zoomable/&gt;
&lt;/figure&gt;
&lt;p&gt;On solving the DP dynamics PDI and unrolling it over all the $K$ steps of iteration yields the privacy guarantee stated in &lt;a href=&quot;#main_theorem&quot;&gt;Theorem 1&lt;/a&gt;. &lt;a href=&quot;#privacy_loss_total&quot;&gt;Figure below&lt;/a&gt; demonstrates how this RDP guarantee for noisy GD converges with the number of iterations $\K$. Through y-axis, we show the $\eps$ guaranteed for noisy GD under a reasonable choice of hyperparameters. The RDP order $\q$ linearly scales the asymptotic guarantee, but does not affect the convergence rate of RDP guarantee. However, the strong convexity parameter $\lambda$ positively affects the asymptotic guarantee as well as the convergence rate; the larger the strong convexity parameter $\lambda$ is, the stronger the asymptotic RDP guarantee and the faster the convergence.&lt;/p&gt;
&lt;figure id=&quot;privacy_loss_total&quot;&gt;
&lt;img class=&quot;img-fluid rounded z-depth-1&quot;  src=&quot;/assets/resized/privacy_loss_total-480x197.png&quot; srcset=&quot;    /assets/resized/privacy_loss_total-480x197.png 480w,/ assets/2021-11-22-DPDynamics/privacy_loss_total.png 676w&quot;   data-zoomable/&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;tightness-analysis&quot;&gt;Tightness Analysis&lt;/h2&gt;
&lt;p&gt;Differential privacy guarantees reflect a &lt;em&gt;bound&lt;/em&gt; on privacy loss on an algorithm; thus, it is very crucial to also have an analysis of their tightness (i.e., how close they are to the exact privacy loss).  We prove that our guarantee in &lt;a href=&quot;#main_theorem&quot;&gt;Theorem 1&lt;/a&gt; is tight. To this end, we construct an instance of the ERM optimization problem, for which we show that the Rényi privacy loss of the noisy GD algorithm grows at an order matching our guarantee in &lt;a href=&quot;#main_theorem&quot;&gt;Theorem 1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is very challenging to lower bound the exact Rényi privacy loss ${\Ren{\q}{\thet_{\K\step}}{\thet_{\K\step}'}}$ in general. This might require having an explicit expression for the probability distribution over the last-iterate parameters $\ptheta_\kk$. Computing a close-form expression is, however, feasible when the loss gradients are linear.  This is due to the fact that, after a sequence of linear transformations and Gaussian noise additions, the parameters follow a Gaussian distribution. Therefore, we construct such an ERM objective, compute the exact privacy loss, and prove the following lower bound.&lt;/p&gt;
&lt;div id=&quot;lower_bound&quot; class=&quot;theorem&quot;&gt;&lt;b&gt;2.&lt;/b&gt;
There exist two neighboring datasets ${\D, \D' \in \Domain}$, a start parameter $\ptheta_0$, and a smooth loss function $\loss{\x}{\ptheta}$ on unconstrained convex set $\C=\mathbb{R}^d$, with a finite total gradient sensitivity $\sen{g}$, such that for any step-size $\step&lt;1$, noise variance $\sig^2&gt;0$, and iteration $\K\in\mathbb{N}$, the privacy loss of $\mathcal{A}_{\text{Noisy-GD}}$ on $\D, \D'$ is lower-bounded by
$$
\Ren{\q}{\thet_{\step\K}}{\thet_{\step\K}'} \geq \frac{\q S_g^2}{4\sig^2 n^2} \left( 1 - e^{-\step\K} \right).
$$
&lt;/div&gt;
&lt;p&gt;We prove this lower bound using the $\ell_2$-squared norm loss as ERM objective: $\underset{\theta\in\mathbb{R}^d}{\min}\sum_{i=1}^n\frac{\lVert\theta-\x_i\rVert_2^2}{n}$. We assume bounded data domain s.t. the gradient has finite sensitivity. With start parameter $\theta_0 = 0^d$, the $k^{\text{th}}$ step parameter $\theta_k$ is distributed as Gaussian with mean ${\mu_k=\step\bar{\x}\sum_{i=0}^{k-1}(1-\step)^i}$ and variance ${\sigma_k^2=\frac{2\eta\sigma^2}{\size^2}\sum_{i=0}^{k-1}(1-\step)^{2i}}$ in each dimension, where $\bar{x}=\sum_{i=1}^n\x_i/n$ is the empirical dataset mean. We explicitly compute the privacy loss at any step $\K$, which is lower bounded by ${\frac{\q S_g^2}{4\sig^2n^2}(1-e^{-\eta\K})}$.&lt;/p&gt;
&lt;p&gt;Our &lt;a href=&quot;#main_theorem&quot;&gt;RDP guarantee&lt;/a&gt; converges fast to $\frac{\alpha S_g^2}{\sig^2n^2}$, which matches the lower bound at every step $\K$, up to a constant of $4$. This immediately shows tightness of our converging RDP guarantee &lt;em&gt;throughout&lt;/em&gt; the training process, for a converging noisy GD algorithm.&lt;/p&gt;
&lt;h2 id=&quot;utility-analysis&quot;&gt;Utility Analysis&lt;/h2&gt;
&lt;p&gt;The randomness, required for satisfying differential privacy, can adversely affect the utility of the trained model.  The standard way to measure the utility of a randomized ERM algorithm (for example, $\mathcal{A}_{\text{Noisy-GD}}$) is to quantify its worst case &lt;em&gt;excess empirical risk&lt;/em&gt;, which is defined as&lt;/p&gt;
&lt;p&gt;$$
\max_{\D \in \Domain}\Expec{}{\Loss_{\D}(\ptheta) - \Loss_{\D}(\ptheta^*)},
$$&lt;/p&gt;
&lt;p&gt;where $\ptheta$ is the output of the randomized algorithm $\mathcal{A}_{\text{Noisy-GD}}$ on $\D$, $\ptheta^*$ is the optimal solution to the standard (no privacy) ERM on $\Loss_\D$, and the expectation is computed over the randomness of the algorithm.&lt;/p&gt;
&lt;p&gt;We provide the &lt;em&gt;optimal&lt;/em&gt; excess empirical risk (utility) of noisy GD algorithm under an $(\eps, \delta)-DP constraint. The notion of &lt;em&gt;optimality&lt;/em&gt; for utility is defined as the smallest upper-bound for excess empirical risk that can be guaranteed under $(\eps, \delta)$-DP constraint by tuning the algorithm's hyperparameters (such as the noise variance $\sig^2$ and the number of iterations $\K$).&lt;/p&gt;
&lt;div class=&quot;theorem&quot; id=&quot;optimal_empirical_risk&quot;&gt;&lt;b&gt;3 (Empirical risk upper bound for $(\eps,\delta)$-DP Noisy GD).&lt;/b&gt; 
For Lipschitz smooth strongly convex loss function $\ell(\theta;\x)$ on a bounded closed convex set $\mathcal{C}\subseteq\thetaspace$, and dataset $\D\in\Domain$ of size $n$, if the step-size $\eta=\frac{\lambda}{2\beta^2}$ and the initial parameter $\theta_0\sim\proj{\C}{\mathcal{N}(0,\frac{2\sig^2}{\lambda} I_d)}$, then the &lt;a href=&quot;#goal-noisygd&quot;&gt;noisy GD Algorithm&lt;/a&gt; is $(\eps,\delta)$-differentially private, and satisfies
$$
	\mathbb{E}[\Loss_D(\theta_{K^*})-\Loss_D(\theta^*)]=O(\frac{\beta d L^2\log(1/\delta)}{\eps^2\lambda^2\size^2}),
$$
by setting noise variance $\sig^2=\frac{8L^2 (\eps+2\log(1/\delta))}{\lambda\eps^2 n^2}$, and number of updates $K^*=\frac{2\beta^2}{\lambda^2}\log(\frac{n^2\eps^2}{4\log(1/\delta) d})$.
&lt;/div&gt;
&lt;p&gt;This utility matches the following theoretical lower bound in Bassily et al.&lt;d-cite key=&quot;bassily2014private&quot;&gt; for the best attainable utility of $(\eps,\delta)$-differentially private algorithms on Lipschitz smooth strongly convex loss functions.&lt;/p&gt;
&lt;div class=&quot;theorem&quot; id=&quot;optimal_empirical_risk&quot;&gt;&lt;b&gt;4 (&lt;d-cite key=&quot;bassily2014private&quot;&gt;&lt;/d-cite&gt;Empirical risk lower bound for $(\eps,\delta)$-DP).&lt;/b&gt; 
Let $\size, d \in \mathbb N$, $\eps &gt; 0$, and $\delta=o(\frac{1}{n})$. For every $(\eps, \delta)$-differentially private algorithm $\mathcal A$ (whose output is denoted by $\theta^{priv}$), then
$$
\mathbb{E}[\Loss_\D(\ptheta^{priv}) - \Loss_\D(\ptheta^*)] = \Omega\left(\min\left\{1, \frac{d}{\eps^2 \size^2}\right\}\right),
$$
where $\ptheta^*$ minimizes a constructed $1$-Lipschitz, $1$-strongly convex objective $\mathcal{L}_D(\theta)$ over convex set $\C$.
&lt;/div&gt;
&lt;figure id=&quot;utility_table&quot;&gt;
&lt;img class=&quot;img-fluid rounded z-depth-1&quot;  src=&quot;/assets/resized/utility_table-800x334.png&quot; srcset=&quot;    /assets/resized/utility_table-480x201.png 480w,    /assets/resized/utility_table-800x334.png 800w,/ assets/2021-11-22-DPDynamics/utility_table.png 967w&quot;   data-zoomable/&gt;
&lt;/figure&gt;
&lt;p&gt;Our utility matches this lower bound upto the constant factor $\log(1/\delta)$, when assuming $\frac{\beta}{\lambda^2}=O(1)$. This improves upon the previous gradient perturbation methods&lt;d-cite key=&quot;bassily2014private,wang2018differentially&quot;&gt;&lt;/d-cite&gt; by a factor of $\log(n)$, and matches the utility of previously know optimal ERM algorithm for Lipschitz smooth strongly convex loss functions, such as objective perturbation&lt;d-cite key=&quot;chaudhuri2011differentially,kifer2012private&quot;&gt;&lt;/d-cite&gt; and output perturbation&lt;d-cite key=&quot;zhang2017efficient&quot;&gt;&lt;/d-cite&gt;. As shown in &lt;a href=&quot;#utility_table&quot;&gt;Table 1&lt;/a&gt;, our utility guarantee for noisy GD is logarithmically better than that for noisy SGD in Bassily et al.&lt;d-cite key=&quot;bassily2014private&quot;&gt;&lt;/d-cite&gt;, although the two algorithms are extremely similar. This is because we use our tight RDP guarantee, while Bassily et al.&lt;d-cite key=&quot;bassily2014private&quot;&gt;&lt;/d-cite&gt; use a composition-based privacy bound. More specifically, noisy SGD needs $n^2$ iterations to achieve the optimal utility, as shown in &lt;a href=&quot;#utility_table&quot;&gt;Table 1&lt;/a&gt;. This number of iterations is large enough for the composition-based privacy bound to grow above our RDP guarantee, thus leaving room for improving privacy utility trade-off. This concludes that our tight privacy guarantee enables providing a superior privacy-utility trade-off, for Lipschitz, strongly convex, and smooth loss functions.&lt;/p&gt;
&lt;p&gt;Our algorithm also has significantly smaller gradient complexity than noisy SGD&lt;d-cite key=&quot;bassily2014private&quot;&gt;&lt;/d-cite&gt;, for strongly convex loss functions, by a factor of ${n}/{\log n}$. We use a (moderately large) constant step-size, thus achieving fast convergence to optimal utility. However, noisy SGD &lt;d-cite key=&quot;bassily2014private&quot;&gt;&lt;/d-cite&gt; uses a decreasing step-size, thus requiring more iterations to reach optimal utility.&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have developed a novel theoretical framework for analyzing the dynamics of privacy loss for noisy gradient descent algorithms.  Our theoretical results show that by hiding the internal state of the training algorithm (over many iterations over the whole data), we can tightly analyze the rate of information leakage throughout training, and derive a bound that is significantly tighter than that of composition-based approaches.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Future Work.&lt;/strong&gt; Our main result is a tight privacy guarantee for Noisy GD on smooth and strongly convex loss functions.  The assumptions are very similar to that of the prior work on privacy amplification by iteration&lt;d-cite key=&quot;feldman2018privacy&quot;&gt;&lt;/d-cite&gt;, and have obvious advantages in enabling the tightness and utility analysis. However, the remaining open challenge is to extend this analysis to non-smooth and non-convex loss functions, and stochastic gradient updates, which are used notably in deep learning.&lt;/p&gt;</content><author><name>Rishav Chourasia*</name></author><category term="jekyll" /><category term="update" /><category term="main" /><summary type="html">$$ \newcommand{\D}{D} % dataset / database \newcommand{\smh}{\beta} % Smoothness parameter \newcommand{\loss}[2]{\ell(#2;\mathbf{#1})} % Small loss \loss; inputs are fine \newcommand{\ptheta}{\theta} % Removed input and shortened \newcommand{\thet}{\Theta} % Removed input and shortened \newcommand{\x}{\mathbf{x}} \newcommand{\K}{K} % Number of steps \newcommand{\T}{T} % Number of steps \newcommand{\kk}{k} \newcommand{\proj}[2]{\Pi_{#1}(#2)} \newcommand{\C}{\mathcal{C}} \newcommand{\sig}{\sigma} \newcommand{\q}{\alpha} \newcommand{\eps}{\varepsilon} % Define epsion \newcommand{\size}{n} % Database size \newcommand{\step}{\eta} \newcommand{\R}{\mathbb{R}} % Notation for Real numbers \newcommand{\Univ}{\mathcal{X}} % Universe of all database records. \newcommand{\thetaspace}{\mathbb{R}^d} \newcommand{\ve}{\mathbf v} \newcommand{\Loss}{\mathcal{L}} % Big loss \Loss; inputs not necessary \newcommand{\Domain}{\Univ^\size} % Set of all possible databases \newcommand{\Ren}[3]{R_{#1}\left(#2\middle\|#3\right)} % Sortened the name \newcommand{\deltW}[1]{\dif{\W{#1}}} \newcommand{\graD}[1]{\nabla #1} \newcommand{\hesS}[1]{\nabla^2 #1} \newcommand{\tra}[1]{\textsf{Tr}\left(#1\right)} \newcommand{\dif}[1]{d #1} \newcommand{\der}[2]{\frac{\dif{#1}}{\dif{#2}}} \newcommand{\doh}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\lapL}[1]{\Delta #1} \newcommand{\divR}[1]{\nabla \cdot #1} \newcommand{\W}[1]{\mathbf{W}_{#1}} \newcommand{\E}{\mathbb{E}} \newcommand{\Expec}[2]{\underset{#1}{\E}\left[#2\right]} \newcommand{\norm}[1]{\left\lVert#1\right\rVert_2} \newcommand{\Fren}[3]{E_{#1}\left(#2\middle\|#3\right)} % Changed name \newcommand{\Gren}[3]{I_{#1}\left(#2\middle\|#3\right)} \newcommand{\sen}[1]{S_{#1}} $$</summary></entry><entry><title type="html">ML Privacy Meter</title><link href="http://0.0.0.0:4000/jekyll/update/2020/09/07/MLPrivacyMeter.html" rel="alternate" type="text/html" title="ML Privacy Meter" /><published>2020-09-07T11:00:00+08:00</published><updated>2020-09-07T11:00:00+08:00</updated><id>http://0.0.0.0:4000/jekyll/update/2020/09/07/MLPrivacyMeter</id><content type="html" xml:base="http://0.0.0.0:4000/jekyll/update/2020/09/07/MLPrivacyMeter.html">&lt;h2 id=&quot;ml-privacy-meter&quot;&gt;ML Privacy Meter&lt;/h2&gt;
&lt;p&gt;When building machine learning models using sensitive data, organizations should ensure that the data processed in such systems is adequately protected. For a safe and secure use of machine learning models, it is important to have a quantitative assessment of the privacy risks of these models, and to make sure that they do not reveal sensitive personal information about their training data. Data Protection regulations, such as GDPR, and AI governance frameworks require personal data to be protected when used in AI systems, and that the users have control over their data and awareness about how it is being used. For projects involving machine learning on personal data, it is mandatory from &lt;a href=&quot;https://gdpr-info.eu/art-35-gdpr/&quot;&gt;Article 35 of GDPR&lt;/a&gt; to perform a Data Protection Impact Assessment (DPIA). Thus, proper mechanisms need to be in place to quantitatively evaluate and verify the privacy of individuals in every step of the data processing pipeline in AI systems.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/privacytrustlab/ml_privacy_meter&quot;&gt;ML Privacy Meter&lt;/a&gt; is a Python library that enables quantifying the privacy risks of machine learning models. The tool provides privacy risk scores which help in identifying data records among the training data that are under high risk of being leaked through the model.&lt;/p&gt;
&lt;img src=&quot;/assets/2020-09-07-MLPrivacyMeter/ml-privacy-meter.png&quot; width=&quot;75%&quot;&gt;
&lt;p&gt;Machine learning models encode information about the datasets on which they are trained. The encoded information is supposed to reflect the general patterns underlying the population data. However, it is commonly observed that these models memorize specific information about some members of their training data [&lt;a href=&quot;#Song17&quot;&gt;Song et al., 2017&lt;/a&gt;] or be tricked to do so [&lt;a href=&quot;#Carlini19&quot;&gt;Carlini et al., 2019&lt;/a&gt;]. Models with high generalization gap as well as the models with high capacity (such as deep neural networks) are more susceptible to memorizing data points from their training set. This is reflected in the predictions of the model, which exhibits a different behavior on training data versus test data, and in the model's parameters which store statistically correlated information about specific data points in their training set. This vulnerability of machine learning models was shown using &lt;em&gt;membership inference attacks&lt;/em&gt;, where an attacker detects the presence of a particular record in the training dataset of a model, just by observing the model. Machine learning models were shown to be susceptible to these attacks in both the black-box [&lt;a href=&quot;#Shokri17&quot;&gt;Shokri et al., 2017&lt;/a&gt;] and white-box settings [&lt;a href=&quot;#Nasr19&quot;&gt;Nasr19 et al., 2019&lt;/a&gt;]. The privacy risks of machine learning models can be evaluated as the accuracy of such inference attacks against their training data.&lt;/p&gt;
&lt;p&gt;In the black-box setting such as machine learning as a service offered on cloud platforms by companies such as &lt;a href=&quot;https://aws.amazon.com/machine-learning&quot;&gt;Amazon&lt;/a&gt;, &lt;a href=&quot;https://studio.azureml.net&quot;&gt;Microsoft&lt;/a&gt;, and &lt;a href=&quot;https://cloud.google.com/prediction&quot;&gt;Google&lt;/a&gt;, we can only observe predictions of the model. This setting models the scenario can be used to measure the privacy risks against legitimate users of a model who seek predictions on their queries. In the white-box setting, we can also observe the parameters of the model. This reflects the scenario where a model is outsourced to a potentially untrusted server or to the cloud, or is shared with an aggregator in the federated learning setting. &lt;strong&gt;ML Privacy Meter&lt;/strong&gt; implements membership inference attacks in both the black-box and white-box settings. Ability to detect membership in the dataset using the released models is a measure of information leakage about the individuals in the dataset from the model.&lt;/p&gt;
&lt;p&gt;Guidances released published by the &lt;a href=&quot;https://ico.org.uk/media/about-the-ico/consultations/2617219/guidance-on-the-ai-auditing-framework-draft-for-consultation.pdf&quot;&gt;Information Commissioner’s Office (ICO) for auditing AI&lt;/a&gt; and the &lt;a href=&quot;https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8269-draft.pdf&quot;&gt;National Institute of Standards and Technology (NIST) for securing applications of Artificial Intelligence&lt;/a&gt; emphasize on the threats to data from models and recommend organizations to account for and estimate these risks to comply with data protection regulations. And they specifically mention membership inference as a confidentiality violation and potential threat to the training data from models. It is recommended in the auditing framework by ICO for organizations to identify these threats and take measures to minimize the risk. As the ICO’s investigation teams will be using this framework to assess the compliance with data protection laws, organizations must account for and estimate the privacy risks to data through models.&lt;/p&gt;
&lt;p&gt;ML privacy meter can help in DPIA by providing a quantitative assessment of privacy risk of a machine learning model. The tool can generate extensive privacy reports about the aggregate and individual risk for data records in the training set at multiple levels of access to the model. It can estimate the amount of information that can be revealed through the predictions of a model (referred to as Black-box access) and through both the predictions and parameters of a model (referred to as White-box access). Hence, when providing query access to the model or revealing the entire model, the tool can be used to assess the potential threats to training data.&lt;/p&gt;
&lt;img src=&quot;/assets/2020-09-07-MLPrivacyMeter/privacy_risk.png&quot; width=&quot;75%&quot;&gt;
&lt;p&gt;ML Privacy Meter works by implementing membership inference attacks against machine learning models. It simulates attackers with different levels of access and knowledge about the model. It considers attackers who can exploit only the predictions of the model, the loss values, and the parameters of the model. For each of the simulated attacks, the tool reports risk scores for all the data records. These scores represent the attacker’s belief that the record was part of the training dataset. The larger the gap between the distribution of these scores for records that are in the training set versus records that are not in the training set, the larger is the leakage from the model would be.&lt;/p&gt;
&lt;img src=&quot;/assets/2020-09-07-MLPrivacyMeter/roc.png&quot; width=&quot;75%&quot;&gt;
&lt;p&gt;Success of the attacker can be quantified by an ROC curve representing the trade-off between False Positive Rate and True Positive Rate of the attacker. True positive represents correctly identifying a member as present in the data and False positive refers to identifying a non-member as member. An attack is successful if it can achieve larger values of True Positive rate at small values of False Positive rate. A trivial attack such as random guess can achieve equal True Positive and False Positive Rates. ML Privacy Meter automatically plots the trade-offs that are achieved by our simulated attackers. The area under those curves quantifies the aggregate privacy risk to the data posed by the model. The higher the area under curve, larger the risk. These numbers not only quantify the success of membership inference attacks, but they can also be seen as a measure of information leakage from the model.&lt;/p&gt;
&lt;img src=&quot;/assets/2020-09-07-MLPrivacyMeter/privacy_risk_label15.png&quot; width=&quot;45%&quot;&gt;
&lt;img src=&quot;/assets/2020-09-07-MLPrivacyMeter/privacy_risk_label45.png&quot; width=&quot;45%&quot;&gt; 
&lt;p&gt;When deploying machine learning models, this quantification of risk can be useful while performing a Data Protection Impact Assessment. The aim of doing a DPIA is to analyze, identify and minimize the potential threats to data. ML privacy meter can guide practitioners in all the three steps. The tool produces detailed privacy reports for the training data. It allows comparing the risk across records from different classes in the data. It can also compare the risk posed by providing black box access to the model with the risk due to white box access. As the tool can immediately measure the privacy risks for training data, practitioners can take simple actions such as finetuning their regularization techniques, sub-sampling, re-sampling their data, etc., to reduce the privacy risk. Or they can even choose to learn with a privacy protection, such as differential privacy, in place.&lt;/p&gt;
&lt;p&gt;Differential Privacy is a cryptographic notion of privacy, wherein the outputs of a computation should be indistinguishable when any single record in the data is modified. The level of indistinguishability is controlled by a privacy parameter $\epsilon$. Open source tools such as &lt;a href=&quot;https://github.com/opendifferentialprivacy/&quot;&gt;OpenDP&lt;/a&gt; and &lt;a href=&quot;https://github.com/tensorflow/privacy&quot;&gt;TensorFlow Privacy&lt;/a&gt; are available for training models with differential privacy guarantees. Selecting an appropriate value for $\epsilon$ is highly non-trivial when using these tools. Models learned with smaller value of $\epsilon$ provide better privacy guarantees but are also less accurate. $\epsilon$ represents a worst case upper bound on the privacy risk and the practical risk might be much lower.&lt;/p&gt;
&lt;p&gt;ML Privacy Meter can help in the selection of privacy parameters ($\epsilon$) for differential privacy by quantifying the risk posed at each value of epsilon. Compared to just relying on the guarantees provided by epsilon, using this method helps in deploying models with higher accuracy. By letting practitioners choose models with better utility, ML Privacy Meter can enable the use of privacy risk minimization techniques.&lt;/p&gt;
&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;By leaking information through predictions and parameters, machine learning models pose an additional privacy risk to data in AI systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To comply with data protection regulations, we need to assess these risks and take possible mitigation measures.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ML Privacy Meter quantifies the privacy risk of machine learning models to their training data and can guide practitioners in regulatory compliance by helping them analyze, identify, and minimize the threats to data. Repository for the code and tutorials is available &lt;a href=&quot;https://github.com/privacytrustlab/ml_privacy_meter&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;a name=&quot;Shokri15&quot;&gt;&lt;/a&gt; Shokri, Reza, and Vitaly Shmatikov. &amp;quot;Privacy-preserving deep learning.&amp;quot; Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. 2015.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Shokri17&quot;&gt;&lt;/a&gt; Shokri, Reza, et al. &amp;quot;Membership inference attacks against machine learning models.&amp;quot; 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Nasr19&quot;&gt;&lt;/a&gt; Nasr, Milad, Reza Shokri, and Amir Houmansadr. &amp;quot;Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Song17&quot;&gt;&lt;/a&gt; C. Song, T. Ristenpart, and V. Shmatikov. Machinelearning models that remember too much. InProceedings of the 2017 ACM SIGSAC Conference onComputer and Communications Security, pages587–601, 2017.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Carlini19&quot;&gt;&lt;/a&gt; N. Carlini, C. Liu, ́U. Erlingsson, J. Kos, and D. Song.The secret sharer:  Evaluating and testing unintendedmemorization in neural networks. In28thUSENIXSecurity Symposium, pages 267–284, 2019.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;McMahan17&quot;&gt;&lt;/a&gt; B. McMahan, E. Moore, D. Ramage, S. Hampson, andB. A. y Arcas. Communication-efficient learning ofdeep networks from decentralized data. InArtificialIntelligence and Statistics, pages 1273–1282, 2017&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/sasi-kumar-murakonda/'&gt;Sasi Kumar Murakonda&lt;/a&gt; and &lt;a href='https://www.comp.nus.edu.sg/~mstrobel/'&gt;Martin Strobel&lt;/a&gt;</name></author><category term="jekyll" /><category term="update" /><category term="main" /><summary type="html">...</summary></entry><entry><title type="html">Data Privacy</title><link href="http://0.0.0.0:4000/jekyll/update/2020/08/06/DataPrivacy.html" rel="alternate" type="text/html" title="Data Privacy" /><published>2020-08-06T12:00:00+08:00</published><updated>2020-08-06T12:00:00+08:00</updated><id>http://0.0.0.0:4000/jekyll/update/2020/08/06/DataPrivacy</id><content type="html" xml:base="http://0.0.0.0:4000/jekyll/update/2020/08/06/DataPrivacy.html">&lt;h2 id=&quot;data-privacy---privacy-preserving-computation&quot;&gt;Data privacy - Privacy preserving computation&lt;/h2&gt;
&lt;p&gt;When releasing statistics about a sensitive dataset, we want to ensure that the privacy of individuals in the dataset is not violated. Releasing even simple statistics (such as averages) about a dataset can reveal information about the specific records in the dataset, for example, the presence of an individual in the dataset [&lt;a href=&quot;#Homer08&quot;&gt;Homer, 2008&lt;/a&gt;]. Given enough such seemingly simple statistics, it is possible to reconstruct the entire dataset with high probability [&lt;a href=&quot;#Dinur03&quot;&gt;Dinur et al., 2003&lt;/a&gt;]. People desire the power to plausibly deny their presence or absence in any given dataset. Hence, the output of a computation should not change by much due to the inclusion of a single individual.  Mathematically rigorous privacy guarantees to individuals are required when releasing computations on sensitive datasets. The level of privacy protection and the success of an attacker trying to obtain information are essentially different narratives of the same story. Therefore, the two important angles to approach this problem are: Developing techniques to automatically protect the privacy of the input data and designing attack algorithms to measure information leakage from the output of a computation.&lt;/p&gt;
&lt;p&gt;Tracing attacks, also known as membership inference attacks, where an attacker infers if a particular record was present in the training dataset just by observing the outputs of a computation, are considered as a measure of the information leakage about the input dataset from the output. Tracing attacks have been extensively studied for summary statistics, where independent statistics (e.g., mean) of attributes of high-dimensional data are released [&lt;a href=&quot;#Homer08&quot;&gt;Homer, 2008&lt;/a&gt;; &lt;a href=&quot;#Sankararaman09&quot;&gt;Sankararaman et al., 2009&lt;/a&gt;; &lt;a href=&quot;#Dwork15&quot;&gt;Dwork et al., 2015&lt;/a&gt;]. Theoretical frameworks are available to analyze the upper bound on the power of these inference attacks [&lt;a href=&quot;#Sankararaman09&quot;&gt;Sankararaman et al., 2009&lt;/a&gt;], and their robustness to noisy statistics [&lt;a href=&quot;#Dwork15&quot;&gt;Dwork et al., 2015&lt;/a&gt;], but only for simple models such as product distributions. These attacks were also tested against machine learning models on ML as a service platforms offered by Google and Amazon, and also in federated learning settings, showing the privacy vulnerability of such systems [&lt;a href=&quot;#Shokri17&quot;&gt;Shokri et al., 2017&lt;/a&gt;; &lt;a href=&quot;#Nasr19&quot;&gt;Nasr et al., 2019&lt;/a&gt;; &lt;a href=&quot;#Melis19&quot;&gt;Melis et al., 2019&lt;/a&gt;]. In its core, learning refers to the estimation of parameters, whether it is the calculation of simple statistics or complex ML models. The difference between these estimated values of a parameter (from the dataset) and true value (estimated from the entire population) is what poses the privacy threat to the dataset. The key to protecting the privacy of an individual record in the dataset is ensuring that its effect on this estimation of parameters is minimal.&lt;/p&gt;
&lt;p&gt;Differential privacy is a widely accepted notion of statistical privacy. This approach requires the output of computation to be more or less unchanged when a single record in the dataset is modified [&lt;a href=&quot;#Dwork06&quot;&gt;Dwork et al., 2006&lt;/a&gt;]. This is generally achieved by randomizing the output of the computation through the addition of noise [&lt;a href=&quot;#Dwork14&quot;&gt;Dwork et al., 2014&lt;/a&gt;]. If a trusted curator that can collect all the records exists, the noise can be added at the output of the computation, while providing privacy guarantees to each record. This is generally referred to as global differential privacy. If no such trusted curator exists, each record/contribution needs to be randomized while estimating the final output. This stronger requirement is referred to as local differential privacy. Although it is always desirable to satisfy the stronger notion of local differential privacy, the utility that can be achieved under the notion of global differential privacy is far greater.&lt;/p&gt;
&lt;p&gt;When using differential privacy for practical applications, handling the trade-off between the accuracy of the outputs and privacy-risk to individual data is a key challenge. Multiple relaxations of the standard definition of differential privacy were proposed to improve utility and achieve better bounds on privacy loss over the composition of multiple computations [&lt;a href=&quot;#Dwork06a&quot;&gt;Dwork et al., 2006&lt;/a&gt;; &lt;a href=&quot;#Dwork16&quot;&gt;Dwork et al., 2016&lt;/a&gt;; &lt;a href=&quot;#Bun16&quot;&gt;Bun and Steinke, 2016&lt;/a&gt;]. Among these, Renyi Differential Privacy (RDP) [&lt;a href=&quot;#Mironov17&quot;&gt;Mironov, 2017&lt;/a&gt;] is widely used because of its ability to provide tight bounds on the composition of tail privacy losses. RDP is based on the notion of Renyi Divergence between the distributions of output computed over two neighboring datasets, as opposed to the notion of Max Divergence used in standard differential privacy. In contrast to the divergence based relaxations of differential privacy, the recently introduced notion of &amp;quot;f-differential privacy&amp;quot; is based on a hypothesis-testing framework [&lt;a href=&quot;#Dong19&quot;&gt;Dong et al., 2019&lt;/a&gt;]. Privacy guarantees are defined in terms of the trade-off between Type-I and Type-II errors that are achievable by the most powerful and informed attacker when inferring about the presence of any individual in the dataset. It is worth emphasizing that these are all just desirable definitions of privacy guarantees. A computation needs to be appropriately randomized to satisfy these definitions.&lt;/p&gt;
&lt;p&gt;A different approach to protecting the privacy of individuals in a sensitive dataset, while also releasing outputs of computations on it, is generating synthetic data from the original dataset and using the synthetic to perform computations [&lt;a href=&quot;#Zhang17&quot;&gt;Zhang et al., 2017&lt;/a&gt;; &lt;a href=&quot;#Bindschaedler17&quot;&gt;Bindschaedler et al., 2017&lt;/a&gt;]. The key challenges in generating synthetic data are achieving scalability (across dimensions of the data) and guaranteeing a decent privacy-accuracy trade-off for a given task. Also, the utility of the generated synthetic data heavily depends on the target dataset and the specific task by which utility is measured. Theoretically, there cannot exist a generic synthetic dataset that can accurately answer all queries beyond a certain limit, while also protecting privacy. Hence, the key challenge here is designing techniques that are practical, scalable, and achieve decent utility levels for a given task, at various levels of privacy guarantees.&lt;/p&gt;
&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;a name=&quot;Homer08&quot;&gt;&lt;/a&gt; Homer, Nils, et al. &amp;quot;Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays.&amp;quot; PLoS Genet 4.8 (2008): e1000167.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dinur03&quot;&gt;&lt;/a&gt; Dinur, Irit, and Kobbi Nissim. &amp;quot;Revealing information while preserving privacy.&amp;quot; Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. 2003.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Sankararaman09&quot;&gt;&lt;/a&gt; Sankararaman, Sriram, Guillaume Obozinski, Michael I. Jordan, and Eran Halperin. &amp;quot;Genomic privacy and limits of individual detection in a pool.&amp;quot; Nature genetics 41, no. 9 (2009): 965-967.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dwork15&quot;&gt;&lt;/a&gt; Dwork, Cynthia, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. &amp;quot;Robust traceability from trace amounts.&amp;quot; In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pp. 650-669. IEEE, 2015.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Shokri17&quot;&gt;&lt;/a&gt; Shokri, Reza, et al. &amp;quot;Membership inference attacks against machine learning models.&amp;quot; 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Nasr19&quot;&gt;&lt;/a&gt; Nasr, Milad, Reza Shokri, and Amir Houmansadr. &amp;quot;Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Melis19&quot;&gt;&lt;/a&gt; Melis, Luca, et al. &amp;quot;Exploiting unintended feature leakage in collaborative learning.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dwork06&quot;&gt;&lt;/a&gt; Dwork, Cynthia, et al. &amp;quot;Calibrating noise to sensitivity in private data analysis.&amp;quot; Theory of cryptography conference. Springer, Berlin, Heidelberg, 2006.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dwork14&quot;&gt;&lt;/a&gt; Dwork, Cynthia, and Aaron Roth. &amp;quot;The algorithmic foundations of differential privacy.&amp;quot; Foundations and Trends in Theoretical Computer Science 9.3-4 (2014): 211-407.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dwork06a&quot;&gt;&lt;/a&gt; C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor, “Our data, ourselves: Privacy via distributed noise generation,” inAdvancesin Cryptography—Eurocrypt ’06.  Springer, 2006, pp. 486–503.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dwork16&quot;&gt;&lt;/a&gt; Dwork, Cynthia, and Guy N. Rothblum. &amp;quot;Concentrated differential privacy.&amp;quot; arXiv preprint arXiv:1603.01887 (2016).&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Bun16&quot;&gt;&lt;/a&gt; Bun, Mark, and Thomas Steinke. &amp;quot;Concentrated differential privacy: Simplifications, extensions, and lower bounds.&amp;quot; In Theory of Cryptography Conference, pp. 635-658. Springer, Berlin, Heidelberg, 2016.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Mironov17&quot;&gt;&lt;/a&gt; Mironov, Ilya. &amp;quot;Rényi differential privacy.&amp;quot; In 2017 IEEE 30th Computer Security Foundations Symposium (CSF), pp. 263-275. IEEE, 2017.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dong19&quot;&gt;&lt;/a&gt; Dong, Jinshuo, Aaron Roth, and Weijie J. Su. &amp;quot;Gaussian differential privacy.&amp;quot; arXiv preprint arXiv:1905.02383 (2019).&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Zhang17&quot;&gt;&lt;/a&gt; Zhang, Jun, et al. &amp;quot;Privbayes: Private data release via bayesian networks.&amp;quot; ACM Transactions on Database Systems (TODS) 42.4 (2017): 1-41.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Bindschaedler17&quot;&gt;&lt;/a&gt; Vincent Bindschaedler, Reza Shokri, and Carl A Gunter. 2017. Plausible deniability for privacy-preserving data synthesis. Proceedings of the VLDB Endowment 10, 5(2017), 481–492.&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/sasi-kumar-murakonda/'&gt;Sasi Kumar Murakonda&lt;/a&gt; and &lt;a href='https://www.comp.nus.edu.sg/~mstrobel/'&gt;Martin Strobel&lt;/a&gt;</name></author><category term="jekyll" /><category term="update" /><category term="main" /><summary type="html">...</summary></entry><entry><title type="html">Federated Learning</title><link href="http://0.0.0.0:4000/jekyll/update/2020/08/06/FederatedLearning.html" rel="alternate" type="text/html" title="Federated Learning" /><published>2020-08-06T11:00:00+08:00</published><updated>2020-08-06T11:00:00+08:00</updated><id>http://0.0.0.0:4000/jekyll/update/2020/08/06/FederatedLearning</id><content type="html" xml:base="http://0.0.0.0:4000/jekyll/update/2020/08/06/FederatedLearning.html">&lt;p&gt;How do we train accurate models on sensitive data provided by multiple participants, while reducing concerns of data privacy and purpose limitation? Federated learning (FL) has emerged as a promising paradigm to address this problem. It enables multiple participants to learn a highly accurate global model while ensuring that their data remains safe on local devices [&lt;a href=&quot;#Shokri15&quot;&gt;Shokri et al., 2015&lt;/a&gt;; &lt;a href=&quot;#McMahan17&quot;&gt;McMahan et al., 2017&lt;/a&gt;]. In federated learning, each participant downloads a global model from the server and improves the model using their local private dataset. The only information that the participants share with the server is the local (gradient) update. These individual updates are aggregated at the server to improve the global model. Hence, a highly accurate model can be learned from multiple private datasets while also ensuring that the data remains with each participant.&lt;/p&gt;
&lt;p&gt;FL is being celebrated as a success that permits learning while also protecting the local private data but just exchanging model gradients can still pose a significant threat to the local data [&lt;a href=&quot;#Nasr19&quot;&gt;Nasr et al., 2019&lt;/a&gt;; &lt;a href=&quot;#Melis19&quot;&gt;Melis et al., 2019&lt;/a&gt;; &lt;a href=&quot;#Zhu19&quot;&gt;Zhu et al., 2019&lt;/a&gt;]. Possible methods to reduce this privacy leakage include sharing fewer gradients, dimensionality reduction of the input space, sharing less sensitive information as local updates [&lt;a href=&quot;#Chang19&quot;&gt;Chang et al., 2019&lt;/a&gt;]. Although these methods are easy to implement and only incur a slight reduction of accuracy, there are no provable privacy guarantees or bounds on private information leakage.&lt;/p&gt;
&lt;p&gt;The widely-accepted and provable defense is differential privacy (DP), which guarantees that the output distributions are close for adjacent datasets. In the FL system, one can choose to provide participant-level differential privacy [&lt;a href=&quot;#Brendan18&quot;&gt;Brendan et al., 2018&lt;/a&gt;], or record-level differential privacy [&lt;a href=&quot;#Abadi16&quot;&gt;Abadi et al., 2016&lt;/a&gt;] guarantees. However, these privacy guarantees come at a significant cost to the accuracy of the learned model. In addition to differential privacy, secure multi-party computation can help to guard against information leakage from the updates of a single user [&lt;a href=&quot;#Bonawitz17&quot;&gt;Bonawitz et al., 2017&lt;/a&gt;], however, this security comes with a large communication cost.&lt;/p&gt;
&lt;p&gt;In an FL system, potentially malicious participants can sabotage the collaborative learning process by manipulating local updates. Existing FL algorithms are not robust to adversarial updates. Even a single party can severely disrupt the global model [&lt;a href=&quot;#Blanchard17&quot;&gt;Blanchard et al., 2017&lt;/a&gt;; &lt;a href=&quot;#Baruch19&quot;&gt;Baruch et al., 2019&lt;/a&gt;; &lt;a href=&quot;#Mhamdi18&quot;&gt;Mhamdi et al., 2018&lt;/a&gt;]. Robust aggregation techniques &lt;a href=&quot;#Blanchard17&quot;&gt;Blanchard et al., 2017&lt;/a&gt;; &lt;a href=&quot;#Mhamdi18&quot;&gt;Mhamdi et al., 2018&lt;/a&gt;; &lt;a href=&quot;#Yin18&quot;&gt;Yin et al., 2018&lt;/a&gt;] can be used as a defense to mitigate this problem. Yet, the high-dimensional nature of modern deep learning models results in a rapid decline of the theoretical error guarantees provided by these aggregation techniques &lt;a href=&quot;#Blanchard17&quot;&gt;Blanchard et al., 2017&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Existing FL frameworks share gradients as local updates with the server, which limits all the participants to train on the same model architecture sent by the server. Also, the gradients are usually of high dimension (same as the size as the model), sometimes even having a million dimensions for large deep learning models. It is worthwhile to explore the question of whether it is essential to share the model's gradients to convey information about the local data. More recent work remodels FL by sharing less sensitive model predictions, which allows collaboration between models with heterogeneous architectures [&lt;a href=&quot;#Chang19&quot;&gt;Chang et al., 2019&lt;/a&gt;] and also circumvents security and privacy issues.&lt;/p&gt;
&lt;p&gt;To summarize, existing FL systems face significant privacy and robustness issues. An open research question is how to build FL systems that provide provable privacy guarantees and are robust to the malicious participants, while also preserving the utility of the learned model.&lt;/p&gt;
&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;a name=&quot;Shokri15&quot;&gt;&lt;/a&gt; Shokri, Reza, and Vitaly Shmatikov. &amp;quot;Privacy-preserving deep learning.&amp;quot; Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. 2015.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;McMahan17&quot;&gt;&lt;/a&gt; McMahan, Brendan, et al. &amp;quot;Communication-efficient learning of deep networks from decentralized data.&amp;quot; Artificial Intelligence and Statistics. 2017.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Chang19&quot;&gt;&lt;/a&gt; Chang, Hongyan, et al. &amp;quot;Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer.&amp;quot; arXiv preprint arXiv:1912.11279 (2019).&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Nasr19&quot;&gt;&lt;/a&gt; Nasr, Milad, Reza Shokri, and Amir Houmansadr. &amp;quot;Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Melis19&quot;&gt;&lt;/a&gt; Melis, Luca, et al. &amp;quot;Exploiting unintended feature leakage in collaborative learning.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Zhu19&quot;&gt;&lt;/a&gt; Zhu, Ligeng, Zhijian Liu, and Song Han. &amp;quot;Deep leakage from gradients.&amp;quot; Advances in Neural Information Processing Systems. 2019.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Brendan18&quot;&gt;&lt;/a&gt; Brendan, M. H., et al. &amp;quot;Learning differentially private recurrent language models.&amp;quot; International conference on learning representations, Vancouver, BC, Canada. Vol. 30. 2018.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Abadi16&quot;&gt;&lt;/a&gt; Abadi, Martin, et al. &amp;quot;Deep learning with differential privacy.&amp;quot; Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. 2016.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Bonawitz17&quot;&gt;&lt;/a&gt; Bonawitz, Keith, et al. &amp;quot;Practical secure aggregation for privacy-preserving machine learning.&amp;quot; Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. 2017.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Blanchard17&quot;&gt;&lt;/a&gt; Blanchard, Peva, Rachid Guerraoui, and Julien Stainer. &amp;quot;Machine learning with adversaries: Byzantine tolerant gradient descent.&amp;quot; Advances in Neural Information Processing Systems. 2017.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Baruch19&quot;&gt;&lt;/a&gt; Baruch, Gilad, Moran Baruch, and Yoav Goldberg. &amp;quot;A little is enough: Circumventing defenses for distributed learning.&amp;quot; Advances in Neural Information Processing Systems. 2019.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Mhamdi18&quot;&gt;&lt;/a&gt;  El Mahdi El Mhamdi, Rachid Guerraoui, and S ́ebastien Louis Alexandre Rouault.  The hidden vulnerability of distributed learning in byzantium.  In International  Conference  on  Machine  Learning, number CONF, 2018.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Yin18&quot;&gt;&lt;/a&gt;[ Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett.  Byzantine-robust distributed learning:  Towards optimal statistical rates.  In International Conference on Machine Learning, pages 5650–5659, 2018.&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/sasi-kumar-murakonda/'&gt;Sasi Kumar Murakonda&lt;/a&gt; and &lt;a href='https://www.comp.nus.edu.sg/~mstrobel/'&gt;Martin Strobel&lt;/a&gt;</name></author><category term="jekyll" /><category term="update" /><category term="main" /><summary type="html">...</summary></entry><entry><title type="html">Trustworthy machine learning</title><link href="http://0.0.0.0:4000/jekyll/update/2020/08/06/trustworthyML.html" rel="alternate" type="text/html" title="Trustworthy machine learning" /><published>2020-08-06T11:00:00+08:00</published><updated>2020-08-06T11:00:00+08:00</updated><id>http://0.0.0.0:4000/jekyll/update/2020/08/06/trustworthyML</id><content type="html" xml:base="http://0.0.0.0:4000/jekyll/update/2020/08/06/trustworthyML.html">&lt;p&gt;The key to building trustworthy AI systems is ensuring that they are robust, fair, interpretable, and can maintain the privacy and confidentiality of sensitive data. AI governance frameworks emphasize on these aspects of AI in the assessment lists for ethical and trustworthy AI. Our research focuses on understanding the trade-offs between different requirements for trust in practical machine learning systems. And especially on quantifying the privacy threats to sensitive data in data processing systems.&lt;/p&gt;
&lt;h2 id=&quot;data-privacy-and-confidentiality&quot;&gt;Data privacy and confidentiality&lt;/h2&gt;
&lt;p&gt;When building machine learning models using sensitive data, organizations should ensure that the data processed is adequately protected. For projects involving machine learning on personal data, it is mandatory from Article 35 of the GDPR to perform a Data Protection Impact Assessment (DPIA). The obvious privacy risk lies in is the exposure of sensitive data &lt;em&gt;during&lt;/em&gt; the computation. Can we trust the party that is running the computation on sensitive data? A more subtle privacy threat is the indirect leakage about data &lt;em&gt;through the output&lt;/em&gt; of computation. Can we trust the party that is observing the output of the computation? The former is generally referred to as protecting confidentiality in computation and the latter as privacy-preserving computation.&lt;/p&gt;
&lt;h3 id=&quot;confidential-computation&quot;&gt;Confidential computation&lt;/h3&gt;
&lt;p&gt;The objective of confidential computing is to evaluate a function on a private dataset, without exposing the input data beyond what is revealed by the output of the computation. This problem is broadly referred to as secure function evaluation (SFE) and depending on the setting, there are two primary ways to handle it. If a single participant wants to outsource computation on sensitive data, without revealing the data, then Homomorphic Encryption (HE) can be used. Whereas, if there are multiple participants and a function needs to be jointly evaluated, then Secure MultiParty Computation (MPC) can be used to evaluate the function, while also ensuring that no party learns about anyone else's dataset.&lt;/p&gt;
&lt;p&gt;Homomorphic Encryption (HE) is an encryption scheme that allows computation over encrypted data directly, without the explicit requirement to decrypt it before performing the computation. Fully homomorphic encryption can be used to evaluate any arbitrary function of any depth [&lt;a href=&quot;#Gentry09&quot;&gt;Gentry, 2008&lt;/a&gt;]. A Secure MultiParty Computation (MPC) protocol ensures that no participant in the protocol learns more than what they could have learned in the presence of a trusted third-party to perform the computation [&lt;a href=&quot;#Evans18&quot;&gt;Evans et al., 2018&lt;/a&gt;]. [&lt;a href=&quot;#Yao86&quot;&gt;Yao, 1986&lt;/a&gt;]'s Garbled Circuit (GC) is one of the first protocols that allowed MPC and forms the foundation of many different MPC protocols today. It allows two parties to securely compute a function that has been converted into a boolean circuit. Although theoretical results about the existence of secure MPC protocols for performing any distributed computational task were provided long ago [&lt;a href=&quot;#Goldreich04&quot;&gt;Goldreich et al., 2004&lt;/a&gt;; &lt;a href=&quot;#Canetti02&quot;&gt;Canetti et al., 2018&lt;/a&gt;], it is still a challenging problem to design protocols that have practical communication, memory, and computational costs.&lt;/p&gt;
&lt;p&gt;In the case of machine learning, designing model architectures that are generic enough to perform well over existing datasets, while at the same time reducing the computational cost of SFE is a tricky task. Current work focuses on understanding the trade-off between the accuracy a model architecture can achieve and its computational performance in an SFE setting. This involves designing techniques such as (but not limited to) replacing computationally expensive operations in SFE with their efficient approximations, devising alternative computation strategies that perform the same operations but utilize sub-operations that are more efficient, and reducing data processing precision by quantizing values.&lt;/p&gt;
&lt;h2 id=&quot;data-privacy-in-machine-learning&quot;&gt;Data Privacy in Machine Learning&lt;/h2&gt;
&lt;p&gt;When we perform any computation on a sensitive dataset (e.g. calculating statistics or training machine learning models), it is important to understand the privacy risk of that computation to the individuals in the dataset. Machine learning models pose a subtle privacy risk to the data by indirectly revealing information through the model's predictions and parameters. Guidances released by the Information Commissioner’s Office (UK) and the National Institute of Standards and Technology (US) emphasize on the threats to data from models and recommend organizations to account for and estimate these risks to comply with data protection regulations.&lt;/p&gt;
&lt;p&gt;How do we define and quantify this privacy risk from machine learning models to the individuals whose data is used to train the model? To do so, we need to distinguish two types of information that we can learn from the model: 1) Information that is generic to members of the population and 2) Information that is specific to members in the training set. Note that members of the training set are also members of the population. Any information that the model reveals about particular members in the training data beyond what it reveals about an arbitrary member of the population is a privacy threat to the training data. A key focus of research is designing inference attacks to quantify this privacy loss.&lt;/p&gt;
&lt;p&gt;Inference attacks on machine learning algorithms fall into two fundamental and related categories: tracing (a.k.a. membership inference) attacks, and reconstruction attacks [&lt;a href=&quot;#Dwork17&quot;&gt;Dwork et al., 2017&lt;/a&gt;].  In a  reconstruction attack, the attacker’s objective is to infer attributes of the records in the training set [&lt;a href=&quot;#Dinur03&quot;&gt;Dinur et al., 2003&lt;/a&gt;; &lt;a href=&quot;#Wang09&quot;&gt;Wang et al., 2009&lt;/a&gt;]. In a membership inference attack, however, the attacker’s objective is to infer if a particular individual data record was included in the training dataset [&lt;a href=&quot;#Homer08&quot;&gt;Homer et al., 2008&lt;/a&gt;; &lt;a href=&quot;#Sankararaman09&quot;&gt;Sankararaman et al., 2009&lt;/a&gt;; &lt;a href=&quot;#Dwork15&quot;&gt;Dwork et al., 2015&lt;/a&gt;; &lt;a href=&quot;#Shokri17&quot;&gt;Shokri et al., 2017&lt;/a&gt;; &lt;a href=&quot;#Nasr19&quot;&gt;Nasr19 et al., 2019&lt;/a&gt;]. Accuracy of inference attacks can be used as metrics to quantify leakage from machine learning models, and also to measure the effectiveness of privacy protection techniques.&lt;/p&gt;
&lt;p&gt;Differential Privacy [&lt;a href=&quot;#Dwork06&quot;&gt;Dwork et al., 2006&lt;/a&gt;] is a notion of privacy, wherein the outputs of a computation should be indistinguishable when any single record in the data is modified. If the training process is differentially private [&lt;a href=&quot;#Shokri15&quot;&gt;Shokri et al., 2015&lt;/a&gt;; &lt;a href=&quot;#Abadi16&quot;&gt;Abadi et al., 2016&lt;/a&gt;], the probability of producing a given model from a training dataset that includes a particular record is close to the probability of producing the same model when this record is not included. Differentially private models are, by construction, secure against inference attacks. One obstacle is that differentially private models may significantly reduce the model’s prediction depending on how strict of a privacy guarantee is required.&lt;/p&gt;
&lt;p&gt;Our tool ML Privacy Meter quantifies the privacy risk to data from models through state-of-the-art membership inference attack techniques. By providing practical estimates of the privacy risk posed for different settings, the meter can help in selecting appropriate parameters for the necessary differential privacy guarantee. The tool can also guide practitioners in regulatory compliance by helping them analyze, identify, and minimize the threats to data, when deploying machine learning models [&lt;a href=&quot;#Murakonda20&quot;&gt;Murakonda et al., 2020&lt;/a&gt;].&lt;/p&gt;
&lt;h2 id=&quot;robustness&quot;&gt;Robustness&lt;/h2&gt;
&lt;p&gt;Can we trust a machine learning model to make critical decisions that can affect individuals' socio-economic status? Are the predictions of these models reliable? The predictive behavior of machine learning models can be maliciously modified by slightly perturbing the distribution of training and/or test data. Models that achieve high accuracy on clean data can be made to learn significantly different decision boundaries with the injection of a small amount of poisoned data [&lt;a href=&quot;#Steinhardt17&quot;&gt;Steinhardt et al., 2017&lt;/a&gt;]. Test data can be slightly perturbed to create adversarial examples, which look similar to the original data, but can make the model misclassify them. Understanding these threats against the functionality of machine learning models and designing systems that are robust to these attacks is one of the core requirements for building trustworthy AI.&lt;/p&gt;
&lt;p&gt;Attackers can corrupt the training data of a machine learning model to achieve different objectives. For example, the attacker might want to degrade the overall test accuracy of the model (called indiscriminate attacks) or increase the loss on specific data points or sub-populations (called targeted attacks). They might even seek to create a backdoor in the learning system to misclassify the backdoor instances as a target label specified by the attacker (called backdoor attacks). Machine learning models were shown to be easily susceptible to these attacks, making their usage in safety-critical systems hard.&lt;/p&gt;
&lt;p&gt;To defend against data poisoning attacks, techniques for achieving training time robustness seek to learn a model that minimizes the out-of-training error even if the training dataset is noisy (or poisoned by an attacker). Data sanitization, also known as outlier detection and anomaly detection, is a very common type of defense against data poisoning attacks [&lt;a href=&quot;#Cretu08&quot;&gt;Cretu et al., 2008&lt;/a&gt;; &lt;a href=&quot;#Chen18&quot;&gt;Chen et al., 2018&lt;/a&gt;; &lt;a href=&quot;#Tran18&quot;&gt;Tran et al., 2018&lt;/a&gt;]. Intuitively,  anomaly detectors can filter poisoning data, if the attacker injects points that are very different from the clean data into the training set. However, attackers can generate points that are very similar to the true data distribution but still successfully mislead the model [&lt;a href=&quot;#Koh18&quot;&gt;Koh et al., 2017&lt;/a&gt;; &lt;a href=&quot;#Steinhardt17&quot;&gt;Steinhardt et al., 2017&lt;/a&gt;]. A testing-stage defense against backdoor attacks reverses the backdoor trigger from the victim model and then fixes the model through retraining or pruning [&lt;a href=&quot;#Wang19&quot;&gt;Wang et al., 2019&lt;/a&gt;]. These defenses do not provide any theoretical guarantees of robustness.&lt;/p&gt;
&lt;p&gt;Adversarial training, a commonly used defense against adversarial examples, aims to ensure that the model produces the same prediction even if the points generated from actual test distribution are slightly modified. The high-level idea is to generate a lot of adversarial examples and explicitly train the model not to be fooled by each of them. Most of the existing adversarial training based defenses do not provide robustness guarantees and demonstrate the robustness property via empirical results. However, adversarial robustness is difficult to measure and many existing defenses have been completely circumvented [&lt;a href=&quot;#Athalye18&quot;&gt;Athalye et al., 2018&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Designing provably robust algorithms for training in presence of poisoning data and for mitigating adversarial examples are still open problems.&lt;/p&gt;
&lt;h2 id=&quot;fairness&quot;&gt;Fairness&lt;/h2&gt;
&lt;p&gt;On May 23rd 2016, the nonprofit news platform ProPublica published an article in which it accused an algorithm used to predict the recidivism risks of criminals in many parts of the U.S to be biased against black defendants [&lt;a href=&quot;#Angwin16&quot;&gt;Angwin et al., 2016&lt;/a&gt;]. The company behind rebutted the claim an argued that the analysis was faulty. The story nevertheless sparked new interest by the machine learning community into the problem of machine fairness and it is by far not the only example of uncovered bias of (machine learning) algorithms. Some famous examples are the work by &lt;a href=&quot;Buolamwini18&quot;&gt;Buolamwini and Gebru, 2018&lt;/a&gt; that demonstrated wide disparities in prediction accuracy, depending on skin type and gender, of commercial gender classification systems and the &lt;a href=&quot;https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G&quot;&gt;news stories&lt;/a&gt; about Amazon stopping the implementation of an AI recruitment tool after it showed bias against women. Problems with (un)fairness of machine learning algorithms can be found in many different application domains and solving them is an active area of research.&lt;/p&gt;
&lt;p&gt;Formally, training a machine learning model refers to finding a set of parameters that optimize an average loss function computed over training dataset. However, the parameters that minimize an overall loss might have very different outcomes for different subpopulations and it is often the majority, for which the most and the best data is available, that benefits most from these models. Before one can address unfairness a formal definition of what it means to be fair is needed. In fact many formal definitions with different focus have been proposed.  Some of these metrics focus on equality across sensitive groups [&lt;a href=&quot;#Calders09&quot;&gt;Calders et al., 2009&lt;/a&gt;; &lt;a href=&quot;#Hardt16&quot;&gt;Hardt et al., 2016&lt;/a&gt;] others focus on the fairness of individuals and are trying to formalize a notion that similar people should be treated similarly [&lt;a href=&quot;#Dwork12&quot;&gt;Dwork et al., 2012&lt;/a&gt;]. Yet another approach is to establish causality [&lt;a href=&quot;#Kusner17&quot;&gt;Kusner et al., 2017&lt;/a&gt;] instead of relying on potentially biased correlations in the data. Many techniques are available to achieve group-based fairness such as pre-processing methods [&lt;a href=&quot;#Madras18&quot;&gt;Madras et al., 2018&lt;/a&gt;; &lt;a href=&quot;#Zemel13&quot;&gt;Zemel et al., 2013&lt;/a&gt;], in-processing methods [&lt;a href=&quot;#Agarwal18&quot;&gt;Agarwal et al., 2018&lt;/a&gt;; &lt;a href=&quot;#Kamishima11&quot;&gt;Kamishima et al., 2011&lt;/a&gt;; &lt;a href=&quot;#Zafar17a&quot;&gt;Zafar et al., 2017&lt;/a&gt;; &lt;a href=&quot;#Zafar17b&quot;&gt;Zafar et al., 2017&lt;/a&gt;], and post-processing methods [&lt;a href=&quot;#Hardt16&quot;&gt;Hardt et al., 2016&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Towards minimizing discrimination against a group, fair machine learning algorithms strive to equalize the behavior of a model across different groups, by imposing a fairness constraint on models. Imposing fairness constraints might come at a cost of the model’s performance and multiple definitions of fairness might not be even compatible with each other [&lt;a href=&quot;#Corbett-Davies17&quot;&gt;Corbett-Davies et al., 2018&lt;/a&gt;; &lt;a href=&quot;#Kleinberg16&quot;&gt;Kleinberg et al., 2016&lt;/a&gt;]. It is shown that achieving equal calibration, false positive rate and false negative rate is impossible, if the fraction of positive labeled examples is different across sensitive groups.&lt;/p&gt;
&lt;p&gt;In our recent work [&lt;a href=&quot;#Hongyan20&quot;&gt;Hongyan et al., 2020&lt;/a&gt;], we show that imposing group-fairness constraints on learning algorithms decreases their robustness to poisoning attacks. An attacker that can only control the sampling and labeling process for a fraction of the training data can significantly degrade the test accuracy of the models learned with fairness constraints. We also show that learning with fairness constraints in presence of such adversarial bias results in a classifier that not only has poor test accuracy but is also potentially more discriminatory on test data. In fact, from a practical perspective, such bias can easily and stealthily be perpetrated in many existing systems, as similar to historical discrimination and/or selection bias. While the goal of fairness is to promote the well-being of the protected groups, surprisingly being fair can cause harm in cases where an unconstrained objective would not when the data changes over time &lt;a href=&quot;#Liu18&quot;&gt;Liu et al., 2018&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hence, an important research direction in FairML is to study the accuracy, robustness guarantees of fair machine learning algorithms, the potential consequences of using such algorithms in presence of adversarially biased data, and how their behavior changes over time. Another interesting and crucial challenge is to design fair learning algorithms that are also robust to poisoning attacks.&lt;/p&gt;
&lt;h2 id=&quot;interpretability&quot;&gt;Interpretability&lt;/h2&gt;
&lt;p&gt;The inherent complexity of machine learning models makes it increasingly difficult to comprehend how and why they make certain classification decisions. Research on interpretable machine learning aims to counteract this development. At least three different significant motivations for interpretable machine learning have been identified [&lt;a href=&quot;#Selbst18&quot;&gt;Selbst et al., 2018&lt;/a&gt;]:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Explanations are necessary to give agency to individuals affected by an automated decision. Decisions based on incorrect data or wrong assumptions can only be corrected if they are understood. Finally, interpretability allows a person to strategically change their behavior and obtain a positive outcome in the future.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An auditor can use explanations to investigate a machine learning model. Explanations might help to improve a model's performance for its initial purpose. They can also be used to discover undesirable side-effects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From a moral point, it is inherently unjust to subject a human to black-box decision making. If a fully automatic algorithmic decision cannot be scrutinized, it automatically violates humans personhood, dignity, and autonomy. However, whether or not such a (human) right to explanation exists is still under discussion and has not been broadly legally established [&lt;a href=&quot;#Wachter17&quot;&gt;Wachter et al., 2017&lt;/a&gt;].&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Techniques to achieve interpretable machine learning can be split into two subareas. The first area focuses on creating machine learning algorithms that are inherently interpretable. Those techniques mainly focus on restricting the type or size of used models. The second area focuses on creating explanations or interpretations for already existing methods. The latter can be divided into approaches focusing on a single decision and methods covering the entire model.&lt;/p&gt;
&lt;p&gt;No clear and widely agreed-upon definition for interpretability exists. The terms explainability, transparency, and interpretability are often interchangeably used. The meaning of interpretability might vary widely from person to person as well among different application areas. Clear definitions will help to develop better-structured evaluation methods and metrics. Conducting comprehensive field tests to see which of the existing methods can obtain the desired results, will help to get from a collection of proposed tools to achieving the goals connected to the above motivations.&lt;/p&gt;
&lt;p&gt;The exploration of interactions between interpretability and other aspects of machine learning like performance, privacy, fairness, and robustness will lead to new insights. While many authors claim that inherently interpretable models come with a significant tradeoff in a model's accuracy, others state that this tradeoff is minimal [&lt;a href=&quot;#Rudin19&quot;&gt;Rudin, 2019&lt;/a&gt;]. There is some evidence that model explanations leak private information about the model's data as well as the model itself [&lt;a href=&quot;#Shokri19&quot;&gt;Shokri, 2019&lt;/a&gt;; &lt;a href=&quot;#Milli19&quot;&gt;Milli, 2019&lt;/a&gt;]. Finally, recent work questions the robustness of popular explanation methods [&lt;a href=&quot;#Slack20&quot;&gt;Slack et al., 2029&lt;/a&gt;].&lt;/p&gt;
&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;a name=&quot;Gentry09&quot;&gt;&lt;/a&gt; Gentry, Craig, and Dan Boneh. A fully homomorphic encryption scheme. Vol. 20. No. 9. Stanford: Stanford university, 2009.  &lt;strong&gt;[&lt;a href=&quot;https://crypto.stanford.edu/craig/craig-thesis.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Evans18&quot;&gt;&lt;/a&gt; Evans, David, Vladimir Kolesnikov, and Mike Rosulek. &amp;quot;A pragmatic introduction to secure multi-party computation.&amp;quot; Foundations and Trends® in Privacy and Security 2.2-3 (2017). &lt;strong&gt;[&lt;a href=&quot;https://par.nsf.gov/servlets/purl/10099282&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Yao86&quot;&gt;&lt;/a&gt; Yao, Andrew Chi-Chih. &amp;quot;How to generate and exchange secrets.&amp;quot; 27th Annual Symposium on Foundations of Computer Science (sfcs 1986). IEEE, 1986. &lt;strong&gt;[&lt;a href=&quot;https://luca-giuzzi.unibs.it/corsi/Support/papers-cryptography/04568207.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Goldreich04&quot;&gt;&lt;/a&gt; Micali, Silvio, Oded Goldreich, and Avi Wigderson. &amp;quot;How to play any mental game.&amp;quot; Proceedings of the Nineteenth ACM Symp. on Theory of Computing, STOC. 1987. &lt;strong&gt;[&lt;a href=&quot;https://www.researchgate.net/profile/Oded_Goldreich/publication/234778924_How_to_play_ANY_mental_game/links/0deec5232112523fc5000000/How-to-play-ANY-mental-game.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Canetti02&quot;&gt;&lt;/a&gt; Canetti, Ran, et al. &amp;quot;Universally composable two-party and multi-party secure computation.&amp;quot; Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. 2002. &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/509907.509980&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dwork17&quot;&gt;&lt;/a&gt; Dwork, Cynthia, et al. &amp;quot;Exposed! a survey of attacks on private data.&amp;quot; (2017). &lt;strong&gt;[&lt;a href=&quot;https://projects.iq.harvard.edu/files/privacytools/files/pdf_02.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dinur03&quot;&gt;&lt;/a&gt; Dinur, Irit, and Kobbi Nissim. &amp;quot;Revealing information while preserving privacy.&amp;quot; Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. 2003 &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/773153.773173&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Wang09&quot;&gt;&lt;/a&gt; Wang, Rui, et al. &amp;quot;Learning your identity and disease from research papers: information leaks in genome wide association study.&amp;quot; Proceedings of the 16th ACM conference on Computer and communications security. 2009. &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1653662.1653726&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Homer08&quot;&gt;&lt;/a&gt; Homer, Nils, et al. &amp;quot;Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays.&amp;quot; PLoS Genet 4.8 (2008): e1000167. &lt;strong&gt;[&lt;a href=&quot;https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000167&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Sankararaman09&quot;&gt;&lt;/a&gt; Sankararaman, Sriram, et al. &amp;quot;Genomic privacy and limits of individual detection in a pool.&amp;quot; Nature genetics 41.9 (2009): 965-967. &lt;strong&gt;[&lt;a href=&quot;https://www.researchgate.net/profile/Guillaume_Obozinski/publication/26761503_Genomic_privacy_and_limits_of_individual_detection_in_a_pool/links/0a85e53b2c737bf426000000/Genomic-privacy-and-limits-of-individual-detection-in-a-pool.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dwork15&quot;&gt;&lt;/a&gt; Dwork, Cynthia, et al. &amp;quot;Robust traceability from trace amounts.&amp;quot; 2015 IEEE 56th Annual Symposium on Foundations of Computer Science. IEEE, 2015. &lt;strong&gt;[&lt;a href=&quot;https://dash.harvard.edu/bitstream/handle/1/34325450/robust.pdf?sequence=2&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Shokri17&quot;&gt;&lt;/a&gt; Shokri, Reza, et al. &amp;quot;Membership inference attacks against machine learning models.&amp;quot; 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017.  &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/1610.05820.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Nasr19&quot;&gt;&lt;/a&gt;Nasr, Milad, Reza Shokri, and Amir Houmansadr. &amp;quot;Comprehensive privacy analysis of deep learning.&amp;quot; 2019 ieee symposium on security and privacy. 2019. &lt;strong&gt;[&lt;a href=&quot;https://www.researchgate.net/profile/Reza_Shokri/publication/331824638_Comprehensive_Privacy_Analysis_of_Deep_Learning_Passive_and_Active_White-box_Inference_Attacks_against_Centralized_and_Federated_Learning/links/5c8e87aba6fdcc38175a648e/Comprehensive-Privacy-Analysis-of-Deep-Learning-Passive-and-Active-White-box-Inference-Attacks-against-Centralized-and-Federated-Learning.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dwork06&quot;&gt;&lt;/a&gt; Dwork, Cynthia, et al. &amp;quot;Calibrating noise to sensitivity in private data analysis.&amp;quot; Theory of cryptography conference. Springer, Berlin, Heidelberg, 2006. &lt;strong&gt;[&lt;a href=&quot;https://link.springer.com/content/pdf/10.1007/11681878_14.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Shokri15&quot;&gt;&lt;/a&gt; Shokri, Reza, and Vitaly Shmatikov. &amp;quot;Privacy-preserving deep learning.&amp;quot; Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. 2015.   &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2810103.2813687?casa_token=YEbcQLGsS3IAAAAA:Xd7W_JtRpEej_Bu-WVKh-SOJz2D6tFxNuSBuzm3jdXpHbyQW79AJN2XO-qAeGvdq0nIJ1lv5ERSoHw&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Abadi16&quot;&gt;&lt;/a&gt; Abadi, Martin, et al. &amp;quot;Deep learning with differential privacy.&amp;quot; Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. 2016.  &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2976749.2978318?casa_token=-CihDYdnNokAAAAA:qSzL86-COhi-Dt5Y1R67KVlt-_dF2jR_aHBPNMD-IXfHoWO5-VkK8KZiMLHUHwGeA409J6iX5R1fyA&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Murakonda20&quot;&gt;&lt;/a&gt; Murakonda, Sasi Kumar, and Reza Shokri. &amp;quot;ML Privacy Meter: Aiding Regulatory Compliance by Quantifying the Privacy Risks of Machine Learning.&amp;quot; arXiv preprint arXiv:2007.09339 (2020). &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/2007.09339&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Steinhardt17&quot;&gt;&lt;/a&gt; Steinhardt, Jacob, Pang Wei W. Koh, and Percy S. Liang. &amp;quot;Certified defenses for data poisoning attacks.&amp;quot; Advances in neural information processing systems. 2017. &lt;strong&gt;[&lt;a href=&quot;http://papers.nips.cc/paper/6943-certified-defenses-for-data-poisoning-attacks.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Cretu08&quot;&gt;&lt;/a&gt; Cretu, Gabriela F., et al. &amp;quot;Casting out demons: Sanitizing training data for anomaly sensors.&amp;quot; 2008 IEEE Symposium on Security and Privacy (sp 2008). IEEE, 2008. &lt;strong&gt;[&lt;a href=&quot;https://academiccommons.columbia.edu/doi/10.7916/D8D79HQ6/download&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Koh18&quot;&gt;&lt;/a&gt; Koh, Pang Wei, Jacob Steinhardt, and Percy Liang. &amp;quot;Stronger data poisoning attacks break data sanitization defenses.&amp;quot; arXiv preprint arXiv:1811.00741 (2018). &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/1811.00741.pdf?source=post_page&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Chen18&quot;&gt;&lt;/a&gt; Chen, Bryant, et al. &amp;quot;Detecting backdoor attacks on deep neural networks by activation clustering.&amp;quot; arXiv preprint arXiv:1811.03728 (2018). &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/1811.03728&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Tran18&quot;&gt;&lt;/a&gt; Tran, Brandon, Jerry Li, and Aleksander Madry. &amp;quot;Spectral signatures in backdoor attacks.&amp;quot; Advances in Neural Information Processing Systems. 2018. &lt;strong&gt;[&lt;a href=&quot;https://papers.nips.cc/paper/8024-spectral-signatures-in-backdoor-attacks.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Wang19&quot;&gt;&lt;/a&gt; Wang, Bolun, et al. &amp;quot;Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019. &lt;strong&gt;[&lt;a href=&quot;https://www.shawnshan.com/files/publication/backdoor-sp19.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Papernot17&quot;&gt;&lt;/a&gt; Papernot, Nicolas, et al. &amp;quot;Practical black-box attacks against machine learning.&amp;quot; Proceedings of the 2017 ACM on Asia conference on computer and communications security. 2017. &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3052973.3053009?casa_token=C0bpGYRETncAAAAA:GZdPE08f52-S_uGOrb-NNWTK8UBN3BqAO4QLXYwYfsksVrU-JY-7GRtppV60bxNDJNdJCXYcMajOnA&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Athalye18&quot;&gt;&lt;/a&gt; Athalye, Anish, Nicholas Carlini, and David Wagner. &amp;quot;Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.&amp;quot; arXiv preprint arXiv:1802.00420 (2018). &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/1802.00420&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Angwin16&quot;&gt;&lt;/a&gt; Angwin, Julia, et al. &amp;quot;Machine bias.&amp;quot; ProPublica, May 23 (2016): 2016. &lt;strong&gt;[&lt;a href=&quot;https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing&quot;&gt;article&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Buolamwini18&quot;&gt;&lt;/a&gt; Buolamwini, Joy, and Timnit Gebru. &amp;quot;Gender shades: Intersectional accuracy disparities in commercial gender classification.&amp;quot; Conference on fairness, accountability and transparency. 2018. &lt;strong&gt;[&lt;a href=&quot;http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Calders09&quot;&gt;&lt;/a&gt; Calders, Toon, Faisal Kamiran, and Mykola Pechenizkiy. &amp;quot;Building classifiers with independency constraints.&amp;quot; 2009 IEEE International Conference on Data Mining Workshops. IEEE, 2009. &lt;strong&gt;[&lt;a href=&quot;https://www.researchgate.net/profile/Toon_Calders/publication/220764898_Building_Classifiers_with_Independency_Constraints/links/0fcfd50f87c5a80eb2000000/Building-Classifiers-with-Independency-Constraints.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Hardt16&quot;&gt;&lt;/a&gt;  Hardt, Moritz, Eric Price, and Nati Srebro. &amp;quot;Equality of opportunity in supervised learning.&amp;quot; Advances in neural information processing systems. 2016. &lt;strong&gt;[&lt;a href=&quot;http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dwork12&quot;&gt;&lt;/a&gt; Dwork, Cynthia, et al. &amp;quot;Fairness through awareness.&amp;quot; Proceedings of the 3rd innovations in theoretical computer science conference. 2012. &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2090236.2090255?casa_token=lJv0poBG-bYAAAAA:UAkOsKKxZePzxN77VCVwY3qr1HV8zu-T1Uu3OWOvRh2AEyuQLHZJFfe7CeSzmHDs9wsrpyp6TBsdkQ&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Kusner17&quot;&gt;&lt;/a&gt; Kusner, Matt J., et al. &amp;quot;Counterfactual fairness.&amp;quot; Advances in neural information processing systems. 2017. &lt;strong&gt;[&lt;a href=&quot;http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Madras18&quot;&gt;&lt;/a&gt; Madras, David, et al. &amp;quot;Learning adversarially fair and transferable representations.&amp;quot; arXiv preprint arXiv:1802.06309 (2018). &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/1802.06309&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Zemel13&quot;&gt;&lt;/a&gt; Zemel, Rich, et al. &amp;quot;Learning fair representations.&amp;quot; International Conference on Machine Learning. 2013. &lt;strong&gt;[&lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v28/zemel13.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Agarwal18&quot;&gt;&lt;/a&gt; Agarwal, Alekh, et al. &amp;quot;A reductions approach to fair classification.&amp;quot; arXiv preprint arXiv:1803.02453 (2018). &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/1803.02453&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Kamishima11&quot;&gt;&lt;/a&gt; Kamishima, Toshihiro, Shotaro Akaho, and Jun Sakuma. &amp;quot;Fairness-aware learning through regularization approach.&amp;quot; 2011 IEEE 11th International Conference on Data Mining Workshops. IEEE, 2011. &lt;strong&gt;[&lt;a href=&quot;https://www.researchgate.net/profile/Toshihiro_Kamishima/publication/220766348_Fairness-aware_Learning_through_Regularization_Approach/links/590201894585156502a2e679/Fairness-aware-Learning-through-Regularization-Approach.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Zafar17a&quot;&gt;&lt;/a&gt; Zafar, Muhammad Bilal, et al. &amp;quot;Fairness constraints: Mechanisms for fair classification.&amp;quot; Artificial Intelligence and Statistics. PMLR, 2017. &lt;strong&gt;[&lt;a href=&quot;http://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Zafar17b&quot;&gt;&lt;/a&gt; Zafar, Muhammad Bilal, et al. &amp;quot;Fairness beyond disparate treatment &amp;amp; disparate impact: Learning classification without disparate mistreatment.&amp;quot; Proceedings of the 26th international conference on world wide web. 2017. &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3038912.3052660?casa_token=450pXKO_pNUAAAAA:9OjYsCFwEa6xIa9UT6GtWVfDs8aAUoV7MLmjwkGYfTLBr1m3SQQ390CVm69RW_orNpteRI2bklQdhQ&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Corbett-Davies17&quot;&gt;&lt;/a&gt; Corbett-Davies, Sam, et al. &amp;quot;Algorithmic decision making and the cost of fairness.&amp;quot; Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining. 2017. &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3097983.3098095?casa_token=-mWPgVlsjDgAAAAA:rdxYT9VN0hNEQ63vyIg3Bl_63DSS_r-sG6GQC9kGWOUJrgIRc2Fz9hm4HIaXn5i6gwaV_MJyUWwpqA&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Kleinberg16&quot;&gt;&lt;/a&gt; Kleinberg, Jon, Sendhil Mullainathan, and Manish Raghavan. &amp;quot;Inherent trade-offs in the fair determination of risk scores.&amp;quot; arXiv preprint arXiv:1609.05807 (2016). &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/1609.05807&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Hongyan20&quot;&gt;&lt;/a&gt; Chang, Hongyan, et al. &amp;quot;On Adversarial Bias and the Robustness of Fair Machine Learning.&amp;quot; arXiv preprint arXiv:2006.08669 (2020). &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/2006.08669&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Liu18&quot;&gt;&lt;/a&gt; Liu, Lydia T., et al. &amp;quot;Delayed impact of fair machine learning.&amp;quot; arXiv preprint arXiv:1803.04383 (2018). &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/1803.04383&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Selbst18&quot;&gt;&lt;/a&gt;  Selbst, Andrew D., and Solon Barocas. &amp;quot;The intuitive appeal of explainable machines.&amp;quot; Fordham L. Rev. 87 (2018): 1085. &lt;strong&gt;[&lt;a href=&quot;https://par.nsf.gov/servlets/purl/10121294&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Wachter17&quot;&gt;&lt;/a&gt; Wachter, Sandra, Brent Mittelstadt, and Luciano Floridi. &amp;quot;Why a right to explanation of automated decision-making does not exist in the general data protection regulation.&amp;quot; International Data Privacy Law 7.2 (2017): 76-99. &lt;strong&gt;[&lt;a href=&quot;https://academic.oup.com/idpl/article-pdf/doi/10.1093/idpl/ipx005/17932196/ipx005.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Rudin19&quot;&gt;&lt;/a&gt; Rudin, Cynthia. &amp;quot;Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.&amp;quot; Nature Machine Intelligence 1.5 (2019): 206-215. Nature Machine Intelligence 1.5 (2019): 206-215. &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/1811.10154&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Shokri19&quot;&gt;&lt;/a&gt; Shokri, Reza, Martin Strobel, and Yair Zick. &amp;quot;Privacy risks of explaining machine learning models.&amp;quot; arXiv preprint arXiv:1907.00164 (2019). &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/pdf/1907.00164&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Milli19&quot;&gt;&lt;/a&gt; Milli, Smitha, et al. &amp;quot;Model reconstruction from model explanations.&amp;quot; Proceedings of the Conference on Fairness, Accountability, and Transparency. 2019. &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3287560.3287562?casa_token=Wqp-vf6BUnkAAAAA:_14jVkHtysZHBE8wtTZ_-ayoBUSTp6VChn0mMSGoogJvKnZdwHr-ILnKvNk88_hcEHEfD6ZUn_OqBw&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Slack20&quot;&gt;&lt;/a&gt; Slack, Dylan, et al. &amp;quot;Fooling lime and shap: Adversarial attacks on post hoc explanation methods.&amp;quot; Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 2020. &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3375627.3375830?casa_token=nzJsQmoEPl4AAAAA:KogRkKs5_Q6ZTjfYlycJSBmZBqyYWVh5HqJJGNgwa9MtUHEtFQ6cvvqIvVHRXmMPY7VnF9KwacpCIg&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/sasi-kumar-murakonda/'&gt;Sasi Kumar Murakonda&lt;/a&gt; and &lt;a href='https://www.comp.nus.edu.sg/~mstrobel/'&gt;Martin Strobel&lt;/a&gt;</name></author><category term="jekyll" /><category term="update" /><category term="main" /><summary type="html">...</summary></entry></feed>