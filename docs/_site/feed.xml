<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-08-07T13:03:26+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Privacy and Trustworthy Machine Learning Research Lab</title><subtitle>Website of the NUS Data Privacy and Trustworthy Machine Learning Research Lab.</subtitle><entry><title type="html">Data Privacy</title><link href="http://localhost:4000/jekyll/update/2020/08/06/DataPrivacy.html" rel="alternate" type="text/html" title="Data Privacy" /><published>2020-08-06T12:00:00+08:00</published><updated>2020-08-06T12:00:00+08:00</updated><id>http://localhost:4000/jekyll/update/2020/08/06/DataPrivacy</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2020/08/06/DataPrivacy.html">&lt;h2 id=&quot;data-privacy-and-confidentiality&quot;&gt;Data privacy and confidentiality&lt;/h2&gt;
&lt;p&gt;When we perform any computation on a sensitive dataset (e.g. calculating statistics or training machine learning models), it is important to understand the privacy risk of that computation to the individuals in the dataset. An obvious direct privacy risk is the exposure of sensitive data &lt;em&gt;during&lt;/em&gt; the computation. A more subtle privacy threat is the indirect leakage about data &lt;em&gt;through the output&lt;/em&gt; of computation. The former is generally referred to as protecting confidentiality in computation and the latter as privacy preserving computation.
Confidential computation&lt;/p&gt;
&lt;p&gt;The objective of confidential computing is to evaluate a function on private datasets of two or more parties, while also ensuring that no party learns about anyone else's dataset, beyond what is revealed by the output of the computation. This problem is broadly referred to as secure function evaluation (SFE) and the two primary ways to handle it are Homomorphic Encryption (HE) and  Secure MultiParty Computation (MPC).&lt;/p&gt;
&lt;p&gt;Homomorphic Encryption (HE) is an encryption scheme that allows computation over encrypted data directly, without the explicit requirement to decrypt it before performing the computation. Fully homomorphic encryption can be used to evaluate any arbitrary function of any depth [1]. A Secure MultiParty Computation (MPC) protocol ensures that no participant in the protocol learns more than what they could have learned in the presence of a trusted third-party to perform the computation [2]. Yao's Garbled Circuit (GC) [3] is one of the first protocols that allowed MPC and forms the foundation of many different MPC protocols today. It allows two parties to securely compute a function that has been converted into a boolean circuit. Although theoretical results about the existence of secure MPC protocols for performing any distributed computational task were provided long ago [4,5], it is still a challenging problem to design protocols that have practical communication, memory, and computational costs.&lt;/p&gt;
&lt;p&gt;In the case of machine learning, designing model architectures that are generic enough to perform well over existing datasets, while at the same time reducing the computational cost of SFE is a tricky task. Two primary approaches to tackle this are: 1) Replacing some operations which are computationally expensive in an SFE environment with their approximations, which are more efficient; or devising alternative computation strategies which perform the same operations but utilize sub-operations that are more efficient in the SFE setting. 2) Reducing data processing precision by quantizing values instead of processing them in full-precision, thereby reducing the computation cost [6]. However, both approaches may come at a cost of accuracy due to the approximations used for reducing computational load for SFE. We work towards understanding the trade-off and an optimal compromise between the accuracy of a model architecture and its computational performance in an SFE setting.&lt;/p&gt;
&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] C. Gentry. A fully homomorphic encryption scheme. AAI3382729, Advisor: D. Boneh, PhD thesis, Stanford, CA, USA, 2009.&lt;/p&gt;
&lt;p&gt;[2]  D. Evans, V. Kolesnikov, and M. Rosulek. A pragmatic introduction to secure multi-party computation. Foundations and Trends in Privacy and Security, vol. 2, no. 2-3, pp. 70–246, 2018.&lt;/p&gt;
&lt;p&gt;[3] A. C. Yao. How to generate and exchange secrets. In 27th Annual Symposium on Foundations of Computer Science (SCFS 1986), Oct. 1986, pp. 162–167.&lt;/p&gt;
&lt;p&gt;[4] O. Goldreich, S. Micali and A. Wigderson. How to Play any Mental Game { A Completeness Theorem for Protocols with Honest Majority. In the 19th STOC, pages 218{229, 1987. Details in Foundations of Cryptography: Volume 2 { Basic Applications (Cambridge University Press 2004), by Oded Goldreich.&lt;/p&gt;
&lt;p&gt;[5] R. Canetti, Y. Lindell, R. Ostrovsky and A. Sahai. Universally Composable Two-Party and Multi-Party Computation. In the 34th STOC, pages 494{503, 2002. Full version available at http://eprint.iacr.org/2002/140.&lt;/p&gt;
&lt;p&gt;[6] M. S. Riazi, M. Samragh, H. Chen, K. Laine, K. Lauter, and F. Koushanfar. XONN: XNOR-based Oblivious Deep Neural Network Inference. In 28th USENIX Security Symposium (USENIX Security 19)&lt;/p&gt;
&lt;h2 id=&quot;privacy-preserving-computation&quot;&gt;Privacy preserving computation&lt;/h2&gt;
&lt;p&gt;Releasing even simple statistics (such as averages) a dataset can reveal the information about the specific records in the dataset, for example, presence of an individual in the dataset [1]. Given enough such seemingly simple statistics, it is possible to reconstruct the entire dataset with high probability [2]. Hence, we should seek to have mathematically rigorous privacy guarantees, when releasing computations on sensitive datasets. The level of privacy protection and success of an attacker are essentially different narratives of the same story. Hence the two important angles to approach this problem are: Developing techniques to automatically protect privacy of the input data and designing attack algorithms to measure information leakage from the output of a computation.&lt;/p&gt;
&lt;p&gt;Differential privacy is a widely accepted notion of statistical privacy, which requires the output of computation to be more or less unchanged when a single record in the dataset is modified [3]. This is generally achieved by randomizing the output of the computation through addition of noise [4]. Handling the trade-offs between accuracy and privacy-risks is a key challenge when using differential privacy for practical applications and modern, complex machine learning methods [5,6]. It is important to design mechanisms that can provide better accuracy for a given privacy guarantee and are easy to use in practical settings.&lt;/p&gt;
&lt;p&gt;Membership inference attacks, where an attacker infers if a particular record was present in the training dataset just by observing the outputs of a computation, are considered as a measure of the information leakage about the input data from the output. These attacks were tested on Machine Learning as a Service platforms offered by Google and Amazon, and also in federated learning settings, showing the privacy vulnerability of such systems [7,8,9]. Our tool ML Privacy Meter [10] quantifies the privacy risk of machine learning models by simulating attackers that perform Membership Inference attacks, assuming different levels of background knowledge and capabilities of the attacker, and is based on state-of-the-art attack techniques.&lt;/p&gt;
&lt;p&gt;A different approach to protecting the privacy of individuals in a sensitive dataset, while also releasing outputs of computations on it is generating synthetic data from the original dataset and using the synthetic to perform computations [11,12]. The key challenges in generating synthetic data are achieving scalability (across dimensions of the data) and guaranteeing a decent privacy-accuracy trade-off for a given task. Also, utility of the generated synthetic data heavily depends on the target dataset. Theoretically there cannot exist a generic synthetic dataset that can accurately answer queries beyond a certain limit. Hence, the key challenge here is designing techniques that are practical, scalable, and achieve decent utility levels for a given task, at various levels of privacy guarantees.&lt;/p&gt;
&lt;h3 id=&quot;references-1&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Homer, Nils, et al. &amp;quot;Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays.&amp;quot; PLoS Genet 4.8 (2008): e1000167.&lt;/p&gt;
&lt;p&gt;[2] Dinur, Irit, and Kobbi Nissim. &amp;quot;Revealing information while preserving privacy.&amp;quot; Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. 2003.&lt;/p&gt;
&lt;p&gt;[3] Dwork, Cynthia, et al. &amp;quot;Calibrating noise to sensitivity in private data analysis.&amp;quot; Theory of cryptography conference. Springer, Berlin, Heidelberg, 2006.&lt;/p&gt;
&lt;p&gt;[4] Dwork, Cynthia, and Aaron Roth. &amp;quot;The algorithmic foundations of differential privacy.&amp;quot; Foundations and Trends in Theoretical Computer Science 9.3-4 (2014): 211-407.&lt;/p&gt;
&lt;p&gt;[5] Shokri, Reza, and Vitaly Shmatikov. &amp;quot;Privacy-preserving deep learning.&amp;quot; Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. 2015.&lt;/p&gt;
&lt;p&gt;[6] Abadi, Martin, et al. &amp;quot;Deep learning with differential privacy.&amp;quot; Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. 2016.&lt;/p&gt;
&lt;p&gt;[7] Shokri, Reza, et al. &amp;quot;Membership inference attacks against machine learning models.&amp;quot; 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017.&lt;/p&gt;
&lt;p&gt;[8] Nasr, Milad, Reza Shokri, and Amir Houmansadr. &amp;quot;Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.&lt;/p&gt;
&lt;p&gt;[9] Melis, Luca, et al. &amp;quot;Exploiting unintended feature leakage in collaborative learning.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.&lt;/p&gt;
&lt;p&gt;[10] Murakonda, Sasi Kumar, and Reza Shokri. &amp;quot;ML Privacy Meter: Aiding Regulatory Compliance by Quantifying the Privacy Risks of Machine Learning.&amp;quot; arXiv preprint arXiv:2007.09339 (2020).&lt;/p&gt;
&lt;p&gt;[11] Zhang, Jun, et al. &amp;quot;Privbayes: Private data release via bayesian networks.&amp;quot; ACM Transactions on Database Systems (TODS) 42.4 (2017): 1-41.&lt;/p&gt;
&lt;p&gt;[12] Vincent Bindschaedler, Reza Shokri, and Carl A Gunter. 2017. Plausible deniability for privacy-preserving data synthesis. Proceedings of the VLDB Endowment 10, 5(2017), 481–492.&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/sasi-kumar-murakonda/'&gt;Sasi Kumar Murakonda&lt;/a&gt; and &lt;a href='https://www.comp.nus.edu.sg/~mstrobel/'&gt;Martin Strobel&lt;/a&gt;</name></author><category term="main" /><summary type="html">...</summary></entry><entry><title type="html">Federated Learning</title><link href="http://localhost:4000/jekyll/update/2020/08/06/FederatedLearning.html" rel="alternate" type="text/html" title="Federated Learning" /><published>2020-08-06T11:00:00+08:00</published><updated>2020-08-06T11:00:00+08:00</updated><id>http://localhost:4000/jekyll/update/2020/08/06/FederatedLearning</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2020/08/06/FederatedLearning.html">&lt;p&gt;How do we train accurate models on sensitive data from multiple participants, while reducing the concerns of data privacy and purpose limitation? In this context, federated learning (FL) has emerged as a promising paradigm to address this problem, which enables all participants to learn a highly accurate global model while ensuring that the data remains safe on local devices [1, 2]. Back to 2015, Shokri et al. [1] introduced  the concept of distributed deep learning and proposed Distributed Selective Stochastic Gradient Descent (DSSGD) protocol, and in 2016, Google further extended distributed deep learning to federated learning (FL), which shares the same spirit as the distributed learning [2]. In federated learning (distributed learning), each participant downloads the global model from the server and improves the model using their local private dataset. The only information that parties share with the server is the local (gradient) update. These individual updates are aggregated at the server to improve the global model. Hence, a highly accurate model can be learned from multiple private datasets while also ensuring that the data remains locally&lt;/p&gt;
&lt;p&gt;Although FL is proposed as a privacy-preserving means that trains a model without sharing local private data, the privacy issue still exists. Recent works have manifested that exchanging model gradients is not sufficient to provide reasonable privacy guarantees [4,5,6]. The possible methods to reduce privacy leakage include sharing fewer gradients, dimensionality reduction of the input space, sharing less sensitive information as local updates [3]. Although these methods are commonly easy to implement and only incur a slight reduction in accuracy,  there is no provable guarantee for the privacy leakage. The widely-accepted and provable defense is differential privacy (DP) which guarantees that the output distributions are close for adjacent datasets. In the FL system, particularly, there are participant-level differential privacy (DP) [7], and record-level differential privacy (DP) [8]. However, the privacy guarantee comes at the cost of a significant accuracy drop of the learned model. In addition to differential privacy, secure multi-party computation can help guard against information leakage from the updates of a single user [9], however, the security comes at the cost of a large communication cost.&lt;/p&gt;
&lt;p&gt;In an FL system, the (potentially malicious) participants can sabotage the collaborative learning process by manipulating local updates attacks. It has been manifested that existing FL algorithms are not robust to adversarial updates [10,11,12] and even a single party can severely disrupt the global model. There are a few defense mechanisms proposed in the literature [10,12,13] that replace the vulnerable aggregation algorithm in FL with more robust aggregation algorithms. However, as shown in [10], the high dimensionality of commonly used deep learning models explode the theoretical error of these robust aggregations and cannot provide robustness guarantee for the global model.&lt;/p&gt;
&lt;p&gt;Most of the existing FL frameworks share gradients as local updates with the server, which is typically limited only to the homogeneous FL architectures. It is worthwhile to explore the question of whether it is essential to share the model's gradients. A more recent work revolutionizes FL by sharing less sensitive model predictions, which allows collaboration between models with heterogeneous architectures [3] and also circumvents security and privacy issues.&lt;/p&gt;
&lt;p&gt;In conclusion, the privacy issue in FL system is still critical. The open research question is how to build FL systems that provide a provable privacy guarantee and also preserve the utility of the learned model. Designing a federated learning system that is robust to the malicious participants' updates and preserves the accuracy of the global model is still an open problem.&lt;/p&gt;
&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Shokri, Reza, and Vitaly Shmatikov. &amp;quot;Privacy-preserving deep learning.&amp;quot; Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. 2015.&lt;/p&gt;
&lt;p&gt;[2] McMahan, Brendan, et al. &amp;quot;Communication-efficient learning of deep networks from decentralized data.&amp;quot; Artificial Intelligence and Statistics. 2017.&lt;/p&gt;
&lt;p&gt;[3] Chang, Hongyan, et al. &amp;quot;Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer.&amp;quot; arXiv preprint arXiv:1912.11279 (2019).&lt;/p&gt;
&lt;p&gt;[4] Nasr, Milad, Reza Shokri, and Amir Houmansadr. &amp;quot;Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.&lt;/p&gt;
&lt;p&gt;[5] Melis, Luca, et al. &amp;quot;Exploiting unintended feature leakage in collaborative learning.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.&lt;/p&gt;
&lt;p&gt;[6] Zhu, Ligeng, Zhijian Liu, and Song Han. &amp;quot;Deep leakage from gradients.&amp;quot; Advances in Neural Information Processing Systems. 2019.&lt;/p&gt;
&lt;p&gt;[7] Brendan, M. H., et al. &amp;quot;Learning differentially private recurrent language models.&amp;quot; International conference on learning representations, Vancouver, BC, Canada. Vol. 30. 2018.&lt;/p&gt;
&lt;p&gt;[8] Abadi, Martin, et al. &amp;quot;Deep learning with differential privacy.&amp;quot; Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. 2016.&lt;/p&gt;
&lt;p&gt;[9] Bonawitz, Keith, et al. &amp;quot;Practical secure aggregation for privacy-preserving machine learning.&amp;quot; Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. 2017.&lt;/p&gt;
&lt;p&gt;[10] Blanchard, Peva, Rachid Guerraoui, and Julien Stainer. &amp;quot;Machine learning with adversaries: Byzantine tolerant gradient descent.&amp;quot; Advances in Neural Information Processing Systems. 2017.&lt;/p&gt;
&lt;p&gt;[11] Baruch, Gilad, Moran Baruch, and Yoav Goldberg. &amp;quot;A little is enough: Circumventing defenses for distributed learning.&amp;quot; Advances in Neural Information Processing Systems. 2019.&lt;/p&gt;
&lt;p&gt;[12]  El Mahdi El Mhamdi, Rachid Guerraoui, and S ́ebastien Louis Alexandre Rouault.  The hidden vulnerability of distributed learning in byzantium.  In International  Conference  on  Machine  Learning, number CONF, 2018.&lt;/p&gt;
&lt;p&gt;[13] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett.  Byzantine-robust distributed learning:  Towards optimal statistical rates.  In International Conference on Machine Learning, pages 5650–5659, 2018.&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/sasi-kumar-murakonda/'&gt;Sasi Kumar Murakonda&lt;/a&gt; and &lt;a href='https://www.comp.nus.edu.sg/~mstrobel/'&gt;Martin Strobel&lt;/a&gt;</name></author><category term="main" /><summary type="html">...</summary></entry><entry><title type="html">Trustworthy machine learning</title><link href="http://localhost:4000/jekyll/update/2020/08/06/trustworthyML.html" rel="alternate" type="text/html" title="Trustworthy machine learning" /><published>2020-08-06T11:00:00+08:00</published><updated>2020-08-06T11:00:00+08:00</updated><id>http://localhost:4000/jekyll/update/2020/08/06/trustworthyML</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2020/08/06/trustworthyML.html">&lt;p&gt;The key to building trustworthy AI systems is ensuring that they are robust, fair, interpretable, and can maintain the privacy and confidentiality of sensitive data. AI governance frameworks emphasize on these aspects of AI in the assessment lists for ethical and trustworthy AI. Our research focuses on  understanding the trade-offs between different requirements for trust in practical machine learning systems. Especially on quantifying the privacy threats to sensitive data in data processing systems.&lt;/p&gt;
&lt;h2 id=&quot;fairness&quot;&gt;Fairness&lt;/h2&gt;
&lt;p&gt;Training a machine learning model refers to finding a set of parameters that optimize an average loss function computed over training dataset. Optimizing for such average prediction accuracy can come at the expense of fairness, as the performance of the model might not be optimal on certain sub-populations. As per the regulatory requirements, it is mandatory to ensure that decision making systems are not inherently biased against certain protected groups, that were historically discriminated against.&lt;/p&gt;
&lt;p&gt;In order to address this problem, multiple definitions of fairness were proposed in the literature. Examples include metric equality across sensitive groups [&lt;a href=&quot;#Calders09&quot;&gt;Calders et al., 2009&lt;/a&gt;; &lt;a href=&quot;#Hardt16&quot;&gt;Hardt et al., 2016&lt;/a&gt;], individual fairness [&lt;a href=&quot;#Dwork12&quot;&gt;Dwork et al., 2012&lt;/a&gt;], causality [&lt;a href=&quot;#Kusner17&quot;&gt;Dwork et al., 2017&lt;/a&gt;], and many techniques to satisfy group-based fairness such as pre-processing methods [&lt;a href=&quot;#Madras18&quot;&gt;Madras et al., 2018&lt;/a&gt;; &lt;a href=&quot;#Zemel13&quot;&gt;Zemel et al., 2013&lt;/a&gt;], in-processing methods [&lt;a href=&quot;#Agarwal18&quot;&gt;Agarwal et al., 2018&lt;/a&gt;; &lt;a href=&quot;#Kamishima11&quot;&gt;Kamishima et al., 2011&lt;/a&gt;; &lt;a href=&quot;#Zafar15&quot;&gt;Zafar et al., 2015&lt;/a&gt;; &lt;a href=&quot;#Zafar17&quot;&gt;Zafar et al., 2017&lt;/a&gt;], and post-processing methods [&lt;a href=&quot;#Hardt16&quot;&gt;Hardt et al., 2016&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Towards minimizing discrimination against a group, fair machine learning algorithms strive to equalize the behavior of a model across different groups, by imposing a fairness constraint on models. Imposing fairness constraints might come at a cost of the model’s performance. The effect of fair classification on accuracy and the compatibility of various definitions with each other have been studied in recent work [&lt;a href=&quot;#Corbett-Davies17&quot;&gt;Corbett-Davies et al., 2018&lt;/a&gt;; &lt;a href=&quot;#Kleinberg16&quot;&gt;Kleinberg et al., 2016&lt;/a&gt;]. It is shown that achieving equal calibration, false positive rate and false negative rate is impossible, if the fraction of positive labeled examples is different across sensitive groups.&lt;/p&gt;
&lt;p&gt;In our recent work [&lt;a href=&quot;#Hongyan20&quot;&gt;Hongyan et al., 2020&lt;/a&gt;], we show that imposing group-fairness constraints on learning algorithms decreases their robustness to poisoning attacks. We specifically provide evidence that an attacker that can only control the sampling and labeling process for a fraction of the training data can significantly degrade the test accuracy of the models learned with fairness constraints. We also show that learning with fairness constraints in presence of such adversarial bias results in a classifier that not only has poor test accuracy but is also potentially more discriminatory on test data. In fact, from a practical perspective, such bias can easily and stealthily be perpetrated in many existing systems, as similar to historical discrimination and/or selection bias.
In a recent development, the research community has also started to pay attention to the temporal impact of fair models. While the goal of fairness is to promote the well-being of the protected groups, &lt;a href=&quot;#Liu19&quot;&gt;Liu et al., 2019&lt;/a&gt; show that being fair can cause harm in cases where an unconstrained objective would not when the data changes over time.&lt;/p&gt;
&lt;p&gt;Hence, an important research direction in FairML is to study the accuracy, robustness guarantees of fair machine learning algorithms, the potential consequences of using such algorithms in presence of adversarially biased data, and how their behavior changes over time.  Another interesting and crucial challenge is to design fair learning algorithms that are also robust to poisoning attacks.&lt;/p&gt;
&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;a name=&quot;Calders09&quot;&gt;&lt;/a&gt; Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers with independency constraints. In 2009 IEEE International Conference on Data Mining Workshops, pages 13–18. IEEE, 2009.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Hardt16&quot;&gt;&lt;/a&gt;  Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In Advances in neural information processing systems, pages 3315–3323, 2016.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Dwork12&quot;&gt;&lt;/a&gt; Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Innovations in Theoretical Computer Science (ITCS), pages 214–226, 2012.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Kusner17&quot;&gt;&lt;/a&gt; Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances in Neural Information Processing Systems, pages 4066–4076, 2017.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Madras18&quot;&gt;&lt;/a&gt; David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. arXiv preprint arXiv:1802.06309, 2018.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Zemel13&quot;&gt;&lt;/a&gt; Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In International Conference on Machine Learning, pages 325–333, 2013.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Agarwal18&quot;&gt;&lt;/a&gt; lekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classification. 2018.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Kamishima11&quot;&gt;&lt;/a&gt; Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops, pages 643–650. IEEE, 2011.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Zafar15&quot;&gt;&lt;/a&gt; Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness constraints: Mechanisms for fair classification. arXiv preprint arXiv:1507.05259, 2015.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Zafar17&quot;&gt;&lt;/a&gt; Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment &amp;amp; disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th international conference on world wide web, pages 1171–1180, 2017.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Corbett-Davies17&quot;&gt;&lt;/a&gt; [12] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, pages 797–806, 2017.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Kleinberg16&quot;&gt;&lt;/a&gt; Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807, 2016.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Hongyan20&quot;&gt;&lt;/a&gt; Chang, Hongyan, et al. &amp;quot;On Adversarial Bias and the Robustness of Fair Machine Learning.&amp;quot; arXiv preprint arXiv:2006.08669 (2020).&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;Liu19&quot;&gt;&lt;/a&gt; Liu, Lydia T., et al. &amp;quot;Delayed impact of fair machine learning.&amp;quot; Proceedings of the 28th International Joint Conference on Artificial Intelligence. AAAI Press, 2019&lt;/p&gt;
&lt;/details&gt;
&lt;h2 id=&quot;interpretability&quot;&gt;Interpretability&lt;/h2&gt;
&lt;p&gt;The inherent complexity of machine learning models makes it increasingly difficult to comprehend how and why they make certain classification decisions. Research on interpretable machine learning aims to counteract this development. At least three different significant motivations for interpretable machine learning have been identified [1]:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Explanations are necessary to give agency to individuals affected by an automated decision. Decisions based on incorrect data or wrong assumptions can only be corrected if they are understood. Finally, interpretability allows a person to strategically change their behavior and obtain a positive outcome in the future.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An auditor can use explanations to investigate a machine learning model. Explanations might help to improve a model's performance for its initial purpose. They can also be used to discover undesirable side-effects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From a moral point, it is inherently unjust to subject a human to black-box decision making. If a fully automatic algorithmic decision cannot be scrutinized, it automatically violates humans personhood, dignity, and autonomy. However, whether or not such a (human) right to explanation exists is still under discussion and has not been broadly legally established [2].&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Techniques to achieve interpretable machine learning can be split into two subareas. The first area focuses on creating machine learning algorithms that are inherently interpretable. Those techniques mainly focus on restricting the type or size of used models. The second area focuses on creating explanations or interpretations for already existing methods. The latter can be divided into approaches focusing on a single decision and methods covering the entire model.&lt;/p&gt;
&lt;p&gt;No clear and widely agreed-upon definition for interpretability exists. The terms explainability, transparency, and interpretability are often interchangeably used. The meaning of interpretability might vary widely from person to person as well among different application areas. Clear definitions will help to develop better-structured evaluation methods and metrics. Conducting comprehensive field tests to see which of the existing methods can obtain the desired results, will help to get from a collection of proposed tools to achieving the goals connected to the above motivations.&lt;/p&gt;
&lt;p&gt;The exploration of interactions between interpretability and other aspects of machine learning like performance, privacy, fairness, and robustness will lead to new insights. While many authors claim that inherently interpretable models come with a significant tradeoff in a model's accuracy, others state that this tradeoff is minimal [3]. There is some evidence that model explanations leak private information about the model's data as well as the model itself [4, 5]. Finally, recent work questions the robustness of popular explanation methods [6].&lt;/p&gt;
&lt;h3 id=&quot;references-1&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Selbst, Andrew D., and Solon Barocas. &amp;quot;The intuitive appeal of explainable machines.&amp;quot; Fordham L. Rev. 87 (2018): 1085.&lt;/p&gt;
&lt;p&gt;[2] Wachter, Sandra, Brent Mittelstadt, and Luciano Floridi. &amp;quot;Why a right to explanation of automated decision-making does not exist in the general data protection regulation.&amp;quot; International Data Privacy Law 7.2 (2017): 76-99.&lt;/p&gt;
&lt;p&gt;[3] Rudin, Cynthia. &amp;quot;Stop explaining black-box machine learning models for high stakes decisions and use interpretable models instead.&amp;quot; Nature Machine Intelligence 1.5 (2019): 206-215.&lt;/p&gt;
&lt;p&gt;[4] Shokri, Reza, Martin Strobel, and Yair Zick. &amp;quot;Privacy risks of explaining machine learning models.&amp;quot; arXiv preprint arXiv:1907.00164 (2019).&lt;/p&gt;
&lt;p&gt;[5] Milli, Smitha, et al. &amp;quot;Model reconstruction from model explanations.&amp;quot; Proceedings of the Conference on Fairness, Accountability, and Transparency. 2019.&lt;/p&gt;
&lt;p&gt;[6] Slack, Dylan, et al. &amp;quot;Fooling lime and shap: Adversarial attacks on post hoc explanation methods.&amp;quot; Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 2020.&lt;/p&gt;
&lt;h2 id=&quot;robustness&quot;&gt;Robustness&lt;/h2&gt;
&lt;p&gt;A common assumption in machine learning is that the training data and the test data follow the same distribution. However, this assumption can be violated in practice, say due to injection of poisoning data into the training set by an attacker or due to corruption of test data (adversarial examples). Understanding these threats against functionality of machine learning models and designing systems that are robust to the attacks is one of the core requirements for building trustworthy AI.&lt;/p&gt;
&lt;p&gt;Machine learning systems are susceptible to data poisoning attacks. Models that achieve high accuracy on clean data can be made to learn significantly different decision boundaries with the injection of a small amount of poisoned data [1]. In indiscriminate attacks, the attacker's objective is to degrade the test accuracy of the model. In targeted attacks, the attacker seeks to impose the loss on specific test data points or small sub-populations. In backdoor attacks, the attacker aims at creating backdoor instances so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the attacker. Instead of attacking the training process, in case of adversarial examples, the attacker modifies test inputs to look similar to clean test examples but can make the model mis-predict them.&lt;/p&gt;
&lt;p&gt;Training time robustness aims to learn a model that minimizes the out-of-training (??) error even if the training dataset is noisy (or poisoned by an attacker). Data sanitization, also known as outlier detection and anomaly detection is a very common type of defense [2,4,5]. In poisoning attacks, the attacker is by definition injecting something into the training dataset that is very different from what it should include. Hence, we can use anomaly detectors to filter out training points that look suspicious. However, it has been shown that poisoning attacks can bypass sanitization defenses [3]. The attacker can generate poisoning points that are very similar to the true data distribution but that still successfully mislead the model. In addition, data sanitization defense can also break down when sanitization rules are created based on the poisoning dataset [1]. Testing-stage defense against backdoor attacks reverses a backdoor trigger from the victim model, and then fixes the model through retraining or pruning [6]. However, these defenses do not provide any theoretical guarantees of robustness.&lt;/p&gt;
&lt;p&gt;The objective of test time robustness is to ensure that the model produces the same prediction for points generated from the actual test distribution, even if the points are slightly modified. Adversarial training is commonly used as a defense method against adversarial examples. The high-level idea is to generate a lot of adversarial examples and explicitly train the model not to be fooled by each of them. Most of the existing adversarial training based defenses do not provide robustness guarantees and demonstrate the robustness property via empirical results. However, as shown in [8], adversarial robustness is difficult to measure and most papers get it wrong enough that the results are meaningless.&lt;/p&gt;
&lt;p&gt;In conclusion, designing provably robust algorithms for training in presence of poisoning data and for mitigating adversarial examples are still open problems.&lt;/p&gt;
&lt;h3 id=&quot;references-2&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Steinhardt, Jacob, Pang Wei W. Koh, and Percy S. Liang. &amp;quot;Certified defenses for data poisoning attacks.&amp;quot; Advances in neural information processing systems. 2017.&lt;/p&gt;
&lt;p&gt;[2] Cretu, Gabriela F., et al. &amp;quot;Casting out demons: Sanitizing training data for anomaly sensors.&amp;quot; 2008 IEEE Symposium on Security and Privacy (sp 2008). IEEE, 2008.&lt;/p&gt;
&lt;p&gt;[3] Koh, Pang Wei, Jacob Steinhardt, and Percy Liang. &amp;quot;Stronger data poisoning attacks break data sanitization defenses.&amp;quot; arXiv preprint arXiv:1811.00741 (2018).&lt;/p&gt;
&lt;p&gt;[4] Chen, Bryant, et al. &amp;quot;Detecting backdoor attacks on deep neural networks by activation clustering.&amp;quot; arXiv preprint arXiv:1811.03728 (2018).&lt;/p&gt;
&lt;p&gt;[5] Tran, Brandon, Jerry Li, and Aleksander Madry. &amp;quot;Spectral signatures in backdoor attacks.&amp;quot; Advances in Neural Information Processing Systems. 2018.&lt;/p&gt;
&lt;p&gt;[6] Wang, Bolun, et al. &amp;quot;Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.&amp;quot; 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.&lt;/p&gt;
&lt;p&gt;[7] Papernot, Nicolas, et al. &amp;quot;Practical black-box attacks against machine learning.&amp;quot; Proceedings of the 2017 ACM on Asia conference on computer and communications security. 2017.&lt;/p&gt;
&lt;p&gt;[8] Athalye, Anish, Nicholas Carlini, and David Wagner. &amp;quot;Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.&amp;quot; arXiv preprint arXiv:1802.00420 (2018).&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/sasi-kumar-murakonda/'&gt;Sasi Kumar Murakonda&lt;/a&gt; and &lt;a href='https://www.comp.nus.edu.sg/~mstrobel/'&gt;Martin Strobel&lt;/a&gt;</name></author><category term="main" /><summary type="html">...</summary></entry></feed>