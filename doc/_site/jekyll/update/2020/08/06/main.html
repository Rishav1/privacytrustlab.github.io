<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Trustworthy machine learning | Data Privacy and Trustworthy Machine Learning Research Lab</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Trustworthy machine learning" />
<meta name="author" content="<a href='https://www.linkedin.com/in/sasi-kumar-murakonda/'>Sasi Kumar Murakonda</a> and <a href='https://www.comp.nus.edu.sg/~mstrobel/'>Martin Strobel</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="…" />
<meta property="og:description" content="…" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2020/08/06/main.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2020/08/06/main.html" />
<meta property="og:site_name" content="Data Privacy and Trustworthy Machine Learning Research Lab" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-06T11:00:00+08:00" />
<script type="application/ld+json">
{"datePublished":"2020-08-06T11:00:00+08:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2020/08/06/main.html"},"url":"http://localhost:4000/jekyll/update/2020/08/06/main.html","author":{"@type":"Person","name":"<a href='https://www.linkedin.com/in/sasi-kumar-murakonda/'>Sasi Kumar Murakonda</a> and <a href='https://www.comp.nus.edu.sg/~mstrobel/'>Martin Strobel</a>"},"description":"…","headline":"Trustworthy machine learning","dateModified":"2020-08-06T11:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Privacy and Trustworthy Machine Learning Research Lab" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Data Privacy and Trustworthy Machine Learning Research Lab</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline" style="font-size: 34px">Trustworthy machine learning</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-08-06T11:00:00+08:00" itemprop="datePublished">
        Aug 6, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name"><a href='https://www.linkedin.com/in/sasi-kumar-murakonda/'>Sasi Kumar Murakonda</a> and <a href='https://www.comp.nus.edu.sg/~mstrobel/'>Martin Strobel</a></span></span></p>

    
<script>
  MathJax = {
    loader: {load: ['[tex]/color']},
    tex: {
      packages: {'[+]': ['color']},
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [             // start/end delimiter pairs for display math
        ['$$', '$$'],
        ['\\[', '\\]']
      ],
      processEscapes: true,
    },
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

    <link rel="stylesheet" href="/assets/css/theorem.css">
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h3 id="data-privacy-and-confidentiality">Data privacy and confidentiality</h3>

<p>When we perform any computation on a sensitive dataset (e.g. calculating statistics or training machine learning models), it is important to understand the privacy risk of that computation to the individuals in the dataset. An obvious direct privacy risk is the exposure of sensitive data <em>during</em> the computation. A more subtle privacy threat is the indirect leakage about data <em>through the output</em> of computation. The former is generally referred to as protecting confidentiality in computation and the latter as privacy preserving computation.</p>

<p>Confidential computation</p>

<p>The key challenge in building confidential computing systems is constructing protocols that operate between two or more private dataset owners and cryptographically ensure that execution of the computation won’t familiarize parties with anyone else’s dataset. This problem is broadly referred to as secure function evaluation (SFE) and there are two primary ways to handle this problem:</p>

<p>Homomorphic Encryption (HE) is an encryption scheme that allows computation over encrypted data directly, without the explicit requirement to decrypt it before performing the computation. Variants of homomorphic encryption allow different types of operations to be performed on encrypted data. Fully homomorphic encryption can be used to evaluate any arbitrary function of any depth [1].</p>

<p>Secure MultiParty Computation (MPC) allows a group of non-trusting individuals to compute any function on their inputs jointly. This neither involves disclosing any participant’s private inputs to each other, nor does it involve a trusted third party who can perform the computation [2]. Yao’s Garbled Circuits (GC) [3] is one of the first protocols that allowed MPC. It allows two parties to securely compute a function which has been converted into a boolean circuit. It forms the foundation of many different MPC protocols involving two or more parties.</p>

<p>Both above techniques are computationally expensive. Encryption, decryption and transmission take considerable additional computation beyond the mathematical operations required for computing the actual function. Several approaches have been proposed to address this issue, keeping in mind the SFE protocols they are targeting. Some of them include: 1) Using a combination of different SFE protocols, depending on performance over different types of operations in the function to be evaluated [4]. 2) Simplifying SFE by simplifying the underlying function (model design) during the training phase itself [].</p>

<p>In the case of machine learning, designing models that are generic enough to perform well over existing datasets, while at the same time reducing the computational cost of SFE is a tricky task. Two primary approaches to tackle this are: 1) Replacing some operations which are computationally expensive in an SFE environment with their approximations, which are more efficient; or devising alternative computation strategies which perform the same operations but utilize sub-operations that are more efficient in the SFE setting. 2) Reducing data processing precision by quantizing values instead of processing them in full-precision, thereby reducing the computation cost [5]. However, both approaches may come at a cost of accuracy as they reduce computational cost. The challenge is to find an optimal compromise between the accuracy of the model and performance in an SFE setting.
References
[1]  D. Evans, V. Kolesnikov, and M. Rosulek. A pragmatic introduction to secure multi-party computation. Foundations and Trends in Privacy and Security, vol. 2, no. 2-3, pp. 70–246, 2018.</p>

<p>[2] C. Gentry. A fully homomorphic encryption scheme. AAI3382729, Advisor: D. Boneh, PhD thesis, Stanford, CA, USA, 2009.</p>

<p>[3] A. C. Yao. How to generate and exchange secrets. In 27th Annual Symposium on Foundations of Computer Science (SCFS 1986), Oct. 1986, pp. 162–167.</p>

<p>[4] C. Juvekar, V. Vaikuntanathan, and A. Chandrakasan. GAZELLE: A low latency framework for secure neural network inference. USENIX Security, 2018.</p>

<p>[5] M. S. Riazi, M. Samragh, H. Chen, K. Laine, K. Lauter, and F. Koushanfar. XONN: XNOR-based Oblivious Deep Neural Network Inference. In 28th USENIX Security Symposium (USENIX Security 19)</p>

<h3 id="privacy-preserving-computation">Privacy preserving computation</h3>

<p>Releasing even simple statistics (such as averages) a dataset can reveal the information about the specific records in the dataset, for example, presence of an individual in the dataset [1]. Given enough such seemingly simple statistics, it is possible to reconstruct the entire dataset with high probability [2]. Hence, we should seek to have mathematically rigorous privacy guarantees, when releasing computations on sensitive datasets. The level of privacy protection and success of an attacker are essentially different narratives of the same story. Hence the two important angles to approach this problem are: Developing techniques to automatically protect privacy of the input data and designing attack algorithms to measure information leakage from the output of a computation.</p>

<p>Differential privacy is a widely accepted notion of statistical privacy, which requires the output of computation to be more or less unchanged when a single record in the dataset is modified [3]. This is generally achieved by randomizing the output of the computation through addition of noise [4]. Handling the trade-offs between accuracy and privacy-risks is a key challenge when using differential privacy for practical applications and modern, complex machine learning methods [5,6]. It is important to design mechanisms that can provide better accuracy for a given privacy guarantee and are easy to use in practical settings.</p>

<p>Membership inference attacks, where an attacker infers if a particular record was present in the training dataset just by observing the outputs of a computation, are considered as a measure of the information leakage about the input data from the output. These attacks were tested on Machine Learning as a Service platforms offered by Google and Amazon, and also in federated learning settings, showing the privacy vulnerability of such systems [7,8,9]. Our tool ML Privacy Meter [10] quantifies the privacy risk of machine learning models by simulating attackers that perform Membership Inference attacks, assuming different levels of background knowledge and capabilities of the attacker, and is based on state-of-the-art attack techniques.</p>

<p>A different approach to protecting the privacy of individuals in a sensitive dataset, while also releasing outputs of computations on it is generating synthetic data from the original dataset and using the synthetic to perform computations [11,12]. The key challenges in generating synthetic data are achieving scalability (across dimensions of the data) and guaranteeing a decent privacy-accuracy trade-off for a given task. Also, utility of the generated synthetic data heavily depends on the target dataset. Theoretically there cannot exist a generic synthetic dataset that can accurately answer queries beyond a certain limit. Hence, the key challenge here is designing techniques that are practical, scalable, and achieve decent utility levels for a given task, at various levels of privacy guarantees.
References</p>

<p>[1] Homer, Nils, et al. “Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays.” PLoS Genet 4.8 (2008): e1000167.</p>

<p>[2] Dinur, Irit, and Kobbi Nissim. “Revealing information while preserving privacy.” Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. 2003.</p>

<p>[3] Dwork, Cynthia, et al. “Calibrating noise to sensitivity in private data analysis.” Theory of cryptography conference. Springer, Berlin, Heidelberg, 2006.</p>

<p>[4] Dwork, Cynthia, and Aaron Roth. “The algorithmic foundations of differential privacy.” Foundations and Trends in Theoretical Computer Science 9.3-4 (2014): 211-407.</p>

<p>[5] Shokri, Reza, and Vitaly Shmatikov. “Privacy-preserving deep learning.” Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. 2015.</p>

<p>[6] Abadi, Martin, et al. “Deep learning with differential privacy.” Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. 2016.</p>

<p>[7] Shokri, Reza, et al. “Membership inference attacks against machine learning models.” 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017.</p>

<p>[8] Nasr, Milad, Reza Shokri, and Amir Houmansadr. “Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning.” 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.</p>

<p>[9] Melis, Luca, et al. “Exploiting unintended feature leakage in collaborative learning.” 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.</p>

<p>[10] Murakonda, Sasi Kumar, and Reza Shokri. “ML Privacy Meter: Aiding Regulatory Compliance by Quantifying the Privacy Risks of Machine Learning.” arXiv preprint arXiv:2007.09339 (2020).</p>

<p>[11] Zhang, Jun, et al. “Privbayes: Private data release via bayesian networks.” ACM Transactions on Database Systems (TODS) 42.4 (2017): 1-41.</p>

<p>[12] Vincent Bindschaedler, Reza Shokri, and Carl A Gunter. 2017. Plausible deniability for privacy-preserving data synthesis. Proceedings of the VLDB Endowment 10, 5(2017), 481–492.</p>

<h3 id="fairness">Fairness</h3>

<p>Training a machine learning model refers to finding a set of parameters that optimize an average loss function computed over training dataset. Optimizing for such average prediction accuracy can come at the expense of fairness, as the performance of the model might not be optimal on certain sub-populations. As per the regulatory requirements, it is mandatory to ensure that decision making systems are not inherently biased against certain protected groups, that were historically discriminated against.</p>

<p>In order to address this problem, multiple definitions of fairness were proposed in the literature. Examples include metric equality across sensitive groups [1, 2], individual fairness [3], causality [4], and many techniques to satisfy group-based fairness such as pre-processing methods [5, 6], in-processing methods [7, 8, 9, 10], and post-processing methods [11].</p>

<p>Towards minimizing discrimination against a group, fair machine learning algorithms strive to equalize the behavior of a model across different groups, by imposing a fairness constraint on models. Pre-processing methods aim at finding a new representation of data such that it retains as much information of input features as possible, except those which can lead to bias. In-processing methods enforce fairness during the training process, for example, by incorporating the fairness constraints into the objective function as a regularization term. Post-processing methods correct the predictions of a given trained model, without modifying the training data or the training process.</p>

<p>Challenges and trade-offs for achieving fairness</p>

<p>Imposing fairness constraints might come at a cost of the model’s performance. The effect of fair classification on accuracy and the compatibility of various definitions with each other have been studied in recent work [12, 13]. It is shown that achieving equal calibration, false positive rate and false negative rate is impossible, if the fraction of positive labeled examples is different across sensitive groups.</p>

<p>In our recent work [14], we show that imposing group-fairness constraints on learning algorithms decreases their robustness to poisoning attacks. We specifically provide evidence that an attacker that can only control the sampling and labeling process for a fraction of the training data can significantly degrade the test accuracy of the models learned with fairness constraints. We also show that learning with fairness constraints in presence of such adversarial bias results in a classifier that not only has poor test accuracy but is also potentially more discriminatory on test data. In fact, from a practical perspective, such bias can easily and stealthily be perpetrated in many existing systems, as similar to historical discrimination and/or selection bias.
In a recent development, the research community has also started to pay attention to the temporal impact of fair models. While the goal of fairness is to promote the well-being of the protected groups, [15] shows that being fair can cause harm in cases where an unconstrained objective would not when the data changes over time.</p>

<p>Hence, an important research direction in FairML is to study the accuracy, robustness guarantees of fair machine learning algorithms, the potential consequences of using such algorithms in presence of adversarially biased data, and how their behavior changes over time.  Another interesting and crucial challenge is to design fair learning algorithms that are also robust to poisoning attacks.
References</p>

<p>[1] Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers with independency constraints. In 2009 IEEE International Conference on Data Mining Workshops, pages 13–18. IEEE, 2009.</p>

<p>[2] Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In Advances in neural information processing systems, pages 3315–3323, 2016.</p>

<p>[3] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Innovations in Theoretical Computer Science (ITCS), pages 214–226, 2012.</p>

<p>[4] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances in Neural Information Processing Systems, pages 4066–4076, 2017.</p>

<p>[5] David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. arXiv preprint arXiv:1802.06309, 2018.</p>

<p>[6] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In International Conference on Machine Learning, pages 325–333, 2013.</p>

<p>[7] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classification. 2018.</p>

<p>[8] Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops, pages 643–650. IEEE, 2011.</p>

<p>[9] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness constraints: Mechanisms for fair classification. arXiv preprint arXiv:1507.05259, 2015.</p>

<p>[10] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment &amp; disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th international conference on world wide web, pages 1171–1180, 2017.</p>

<p>[11] Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In Advances in neural information processing systems, pages 3315–3323, 2016.</p>

<p>[12] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, pages 797–806, 2017.</p>

<p>[13] Jon M. Kleinberg, Sendhil Mullainathan, and “ Manish Raghavan. Inherent trade-offs in the fair determination of risk scores.</p>

<p>[14] Chang, Hongyan, et al. “On Adversarial Bias and the Robustness of Fair Machine Learning.” arXiv preprint arXiv:2006.08669 (2020).</p>

<p>[15] Liu, Lydia T., et al. “Delayed impact of fair machine learning.” Proceedings of the 28th International Joint Conference on Artificial Intelligence. AAAI Press, 2019</p>

<h3 id="interpretability">Interpretability</h3>
<p>The inherent complexity of machine learning models makes it increasingly difficult to comprehend how and why they make certain classification decisions. Research on interpretable machine learning aims to counteract this development. At least three different significant motivations for interpretable machine learning have been identified [1]:</p>

<p>1) Explanations are necessary to give agency to individuals affected by an automated decision. Decisions based on incorrect data or wrong assumptions can only be corrected if they are understood. Finally, interpretability allows a person to strategically change their behavior and obtain a positive outcome in the future.</p>

<p>2) An auditor can use explanations to investigate a machine learning model. Explanations might help to improve a model’s performance for its initial purpose. They can also be used to discover undesirable side-effects.</p>

<p>3) From a moral point, it is inherently unjust to subject a human to black-box decision making. If a fully automatic algorithmic decision cannot be scrutinized, it automatically violates humans personhood, dignity, and autonomy. However, whether or not such a (human) right to explanation exists is still under discussion and has not been broadly legally established [2].</p>

<p>Techniques to achieve interpretable machine learning can be split into two subareas. The first area focuses on creating machine learning algorithms that are inherently interpretable. Those techniques mainly focus on restricting the type or size of used models. The second area focuses on creating explanations or interpretations for already existing methods. The latter can be divided into approaches focusing on a single decision and methods covering the entire model.</p>

<p>No clear and widely agreed-upon definition for interpretability exists. The terms explainability, transparency, and interpretability are often interchangeably used. The meaning of interpretability might vary widely from person to person as well among different application areas. Clear definitions will help to develop better-structured evaluation methods and metrics. Conducting comprehensive field tests to see which of the existing methods can obtain the desired results, will help to get from a collection of proposed tools to achieving the goals connected to the above motivations.</p>

<p>The exploration of interactions between interpretability and other aspects of machine learning like performance, privacy, fairness, and robustness will lead to new insights. While many authors claim that inherently interpretable models come with a significant tradeoff in a model’s accuracy, others state that this tradeoff is minimal [3]. There is some evidence that model explanations leak private information about the model’s data as well as the model itself [4, 5]. Finally, recent work questions the robustness of popular explanation methods [6].
References</p>

<p>[1] Selbst, Andrew D., and Solon Barocas. “The intuitive appeal of explainable machines.” Fordham L. Rev. 87 (2018): 1085.</p>

<p>[2] Wachter, Sandra, Brent Mittelstadt, and Luciano Floridi. “Why a right to explanation of automated decision-making does not exist in the general data protection regulation.” International Data Privacy Law 7.2 (2017): 76-99.</p>

<p>[3] Rudin, Cynthia. “Stop explaining black-box machine learning models for high stakes decisions and use interpretable models instead.” Nature Machine Intelligence 1.5 (2019): 206-215.</p>

<p>[4] Shokri, Reza, Martin Strobel, and Yair Zick. “Privacy risks of explaining machine learning models.” arXiv preprint arXiv:1907.00164 (2019).</p>

<p>[5] Milli, Smitha, et al. “Model reconstruction from model explanations.” Proceedings of the Conference on Fairness, Accountability, and Transparency. 2019.</p>

<p>[6] Slack, Dylan, et al. “Fooling lime and shap: Adversarial attacks on post hoc explanation methods.” Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 2020.</p>

<h3 id="robustness">Robustness</h3>
<p>Machine learning is based on the assumption that the data used to train a model is representative of the data the model interacts with during its deployment. Formally, the assumption is that all data comes from the same distribution. In practice however, this premise might not hold for several reasons. 1) The training dataset is noisy and can not reflect the unknown distribution that we aim to learn. For instance, there could be an attacker who controls a small fraction of the training dataset and injects poisoning data into it. 2)  After the model is trained, at test time, test data can be corrupted by malicious users. For instance, a trespasser can try to fool the face recognition system.
Training time robustness
Current machine learning models are vulnerable to so-called poisoning attacks []. For such an attack, the attacker generates a poisoned dataset to degrade the performance of a model on the entire distribution (indiscriminate attack) or on specific examples or small sub-population (targeted attack).  Models that correctly achieve very high accuracy on clean data can be poisoned to learn significantly different decision boundaries with the injection of a small amount of poisoned data [1]. Training time robustness aims to learn a model that minimizes the out-of-training error even if the training dataset is noisy (or poisoned by an attacker).</p>

<p>Data sanitization, also known as outlier detection and anomaly detection is a very common type of defense [2]. The high-level idea is simple: in the poisoning attack, the attacker is by definition injecting something into the training dataset that is very different from what it should include. Hence, we can use anomaly detectors to filter out training points that look suspicious. However, it has been shown that poisoning attacks can bypass sanitization defenses [3]. The attacker can generate poisoning points that are very similar to the true data distribution but that still successfully mislead the model. In addition, data sanitization defense can also break down when sanitization rules are created based on the poisoning dataset [1].</p>

<p>Backdoor attacks are a type of poisoning attack in which the attacker aims at creating backdoor instances so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the attacker. The training-stage defense methods [4,5] use outlier detection to find and then remove the poisoned data. The testing-stage defense [6] reverses a backdoor trigger from a backdoored model, and then fixes the model through retraining or pruning. However, these defenses do not provide any theoretical guarantees of robustness.</p>

<p>Test time robustness</p>

<p>The objective of test time robustness is to ensure that the model produces the same prediction for points generated from the actual test distribution, even if the points are slightly modified. The critical motivation of studying test time robustness is to defend against adversarial examples (evasion attacks). Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake. Adversarial examples could be dangerous. For instance, as discussed in [7], attackers could target autonomous vehicles by using stickers or paint to create an adversarial stop sign such that the vehicle would interpret as a ‘speed limit’ or other sign. Hence, it is urgent to build a robust machine learning model that is adversarially robust (test time robust).</p>

<p>Adversarial training is commonly used as a defense method against adversarial examples. The high-level idea is to generate a lot of adversarial examples and explicitly train the model not to be fooled by each of them. Most of the existing adversarial training based defenses do not provide robustness guarantees and demonstrate the robustness property via empirical results. However, as shown in [8], adversarial robustness is difficult to measure and most papers get it wrong enough that the results are meaningless.</p>

<p>In conclusion, data poisoning attacks pose risks to machine learning systems deployed in the real world. Hence, it is important to design robust training algorithms that are resilient to poisoning attacks. In short, designing adversarial robust models is still an open problem.
References</p>

<p>[1] Steinhardt, Jacob, Pang Wei W. Koh, and Percy S. Liang. “Certified defenses for data poisoning attacks.” Advances in neural information processing systems. 2017.</p>

<p>[2] Cretu, Gabriela F., et al. “Casting out demons: Sanitizing training data for anomaly sensors.” 2008 IEEE Symposium on Security and Privacy (sp 2008). IEEE, 2008.</p>

<p>[3] Koh, Pang Wei, Jacob Steinhardt, and Percy Liang. “Stronger data poisoning attacks break data sanitization defenses.” arXiv preprint arXiv:1811.00741 (2018).</p>

<p>[4] Chen, Bryant, et al. “Detecting backdoor attacks on deep neural networks by activation clustering.” arXiv preprint arXiv:1811.03728 (2018).</p>

<p>[5] Tran, Brandon, Jerry Li, and Aleksander Madry. “Spectral signatures in backdoor attacks.” Advances in Neural Information Processing Systems. 2018.</p>

<p>[6] Wang, Bolun, et al. “Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.” 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.</p>

<p>[7] Papernot, Nicolas, et al. “Practical black-box attacks against machine learning.” Proceedings of the 2017 ACM on Asia conference on computer and communications security. 2017.</p>

<p>[8] Athalye, Anish, Nicholas Carlini, and David Wagner. “Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.” arXiv preprint arXiv:1802.00420 (2018).</p>

<h3 id="federated-learning">Federated Learning</h3>
<p>How do we train accurate models on sensitive data from multiple participants, while reducing the concerns of data privacy and purpose limitation. In this context, federated learning (FL) has emerged as a promising paradigm to address this problem, which enables all participants to learn a highly accurate global model while ensuring that the data remains safe on local devices [1,2].
Back to 2015, Shokri et al. [1] introduced  the concept of distributed deep learning and proposed Distributed Selective Stochastic Gradient Descent (DSSGD) protocol, and in 2016, Google further extended distributed deep learning to federated learning (FL), which shares the same spirit as the distributed learning [2]. In federated learning (distributed learning), each participant downloads the global model from the server and improves the model using their local private dataset. The only information that parties share with the server is the local updates. The party individual updates are aggregated along with other parties’ updates to improve the global model. Hence, the high accuracy model is learned from local private datasets while the private data remains locally. Particularly, the update in federated learning is represented by the gradient of the global model computed on local datasets.</p>

<p>Most of the existing FL frameworks share gradients as local updates with the server, which is typically limited only to the homogeneous FL architectures. It is worthwhile to explore whether sharing the model’s gradients is essential. A more recent work revolutionizes FL by sharing less sensitive model predictions, which allows collaboration between models with heterogeneous architectures [3] and also circumvents security and privacy issues.</p>

<p>Challenges - Privacy
Although FL is proposed as a privacy-preserving means that trains a model without sharing local private data, the privacy issue still exists. Recent works have manifested that exchanging model gradients is not sufficient to provide reasonable privacy guarantees [4,5,6,7].</p>

<p>The objective of the membership inference attack is to infer the membership of a particular target data in the private training dataset. In FL system, any participant or the server might be honest-but-curious even malicious, who can launch inference attack to infer if a particular sample belongs to the private training data of a single party (if the server is malicious) or of any party (if the server is honest but some participants are malicious) [4,5].</p>

<p>The possible methods to reduce privacy leakage include sharing fewer gradients, dimensionality reduction of the input space, sharing less sensitive information as local updates [3,5]. Although these methods are commonly easy to implement and only incur a slight reduction in accuracy,  there is no provable guarantee for the privacy leakage. The widely-accepted and provable defense is differential privacy (DP) which guarantees that the output distributions are close for adjacent datasets. In the FL system, particularly, there are participant-level differential privacy (DP) [2,6], and record-level differential privacy (DP) [5]. However, in the FL system, the privacy guarantee comes at the cost of a significant accuracy drop of the learned model. In addition to differential privacy, secure multi-party computation can help guard against information leakage from the updates of a single user [8,9], however, the security comes at the cost of a large communication cost.</p>

<p>Recent works show that, through shared gradients, the attacks can also reconstruct the private data[7], infer the value of the sensitive attribute, and properties and the training data [5].</p>

<p>Challenges - Robustness
In the FL system, the (potentially malicious) participants can sabotage the collaborative learning process by manipulating local updates attacks.  Poisoning attacks (Byzantine problem) and Defenses. When the attacker (malicious participant) aims to reduce the global model’s accuracy, the problem is studied as a Byzantine problem in the distributed system which is also similar to the poisoning attack in the machine learning context.</p>

<p>The ways of manipulating can be sourced from: (1) data poisoning attack during local dataset collection; and (2) model poisoning attack during local model training process. At a high level, both poisoning attacks attempt to modify the behavior of the target model in some undesirable way. It has been manifested that existing FL algorithms are not robust to adversarial updates [10,11] and even a single party can severely disrupt the global model.</p>

<p>There are a few defense mechanisms proposed in the literature [] that replace the vulnerable aggregation algorithm in FL with more robust aggregation algorithms. However, as shown in [], the high dimensionality of commonly used deep learning models explode the theoretical error of these robust aggregations and cannot provide robustness guarantee for the global model []. As a result, these algorithms are shown to be susceptible to poisoning attacks [].</p>

<p>Besides poisoning attacks, FL systems are susceptible to backdoor attacks, where the attacker’s (malicious participant) goal is to create a backdoor into the global model, so that he can easily circumvent the system by leveraging the backdoor. Specifically, the attacker aims at creating backdoor instances, so that the global model will be misled to classify the backdoor instances as a target label specified by the attacker.</p>

<p>Challenges - Communication Efficiency
In centralized optimization, communication costs are relatively small, and computational costs dominate, with much of the recent emphasis being on using GPUs to lower these costs. In contrast, in federated optimization communication costs dominate. Up to now, it is still difficult to deploy practical federated learning applications in the real world. To alleviate communication burden on the network, two key aspects need to be considered: (i) reducing the total number of communication rounds [], or (ii) reducing the size of transmitted messages at each round [].</p>

<p>Challenges - Multi Task Learning
In settings where data is distributed in a non-iid (not independent and identically distributed) fashion - as is typical in real world situations - the global model produced by FL suffers in terms of test accuracy and/or communication costs compared to training on iid data. Hence, unbalanced and non-IID data partitioning across a massive number of unreliable participants introduces a set of challenges.</p>

<p>In conclusion, the privacy issue in FL system is still critical. The open research question is how to build FL systems that provide a provable privacy guarantee and also preserve the utility of the learned model. Designing a federated learning system that is robust to the malicious participants’ updates and preserves the accuracy of the global model is still an open problem.</p>

  </div><a class="u-url" href="/jekyll/update/2020/08/06/main.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>
  
  <div class="wrapper">
  
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Data Privacy and Trustworthy Machine Learning Research Lab</li><a class="u-email" href="/">Main page</a>
        </ul>
        <!--
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      -->
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Website of the NUS Data Privacy and Trustworthy Machine Learning Research Lab.</p>
      </div>
    </div>

  </div>

</footer></body>

</html>
